{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe331513930>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        lines: [['hello', 'world'], ...]\n",
    "        labels: [[!], [N], ...]\n",
    "        vocab\n",
    "    \"\"\"\n",
    "    with open(file, 'rt') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n\\n')\n",
    "    ret_lines = []\n",
    "    labels = []\n",
    "    vocab = set()\n",
    "    for line in lines:\n",
    "        if not line: \n",
    "            continue\n",
    "        curr_line = []\n",
    "        for token_label_str in line.split('\\n'):\n",
    "            if not token_label_str: \n",
    "                continue\n",
    "            token, label = token_label_str.split('\\t')\n",
    "            vocab.add(token)\n",
    "            labels.append(label)\n",
    "            curr_line.append(token)\n",
    "        ret_lines.append(curr_line)\n",
    "    return ret_lines, labels, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lines(lines, word2idx_map, window_size):\n",
    "    \"\"\"\n",
    "    returns X: len(lines) x (2 * window_size + 1)\n",
    "    \"\"\"\n",
    "    def encode_line(line, word2idx_map, window_size):\n",
    "        num_repr = [] # numerical representation\n",
    "        for word in line:\n",
    "            num = word2idx_map.get(word, word2idx_map['UUUNKKK'])\n",
    "            num_repr.append(num)\n",
    "        # pad with start and end tokens\n",
    "        start = [word2idx_map['<s>']] * window_size\n",
    "        end = [word2idx_map['</s>']] * window_size\n",
    "        padded = start + num_repr + end\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(window_size, len(padded) - window_size):\n",
    "            windowed = padded[i - window_size : i + window_size + 1]\n",
    "            ret.append(windowed)\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    res = []\n",
    "    for line in lines:\n",
    "        res.extend(encode_line(line, word2idx_map, window_size))\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(lines, features):\n",
    "    \"\"\"\n",
    "    features: a set of characters to look for in each token\n",
    "    also append to the end:\n",
    "        (count features are normalized during training)\n",
    "        - a count feature for digits\n",
    "        - a count feature for the length of the entire token\n",
    "    returns X: len(lines) x num_features\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            word_features = []\n",
    "            char_set = set(word)\n",
    "            for feature in features:\n",
    "                val = 1 if feature in char_set else 0\n",
    "                word_features.append(val)\n",
    "\n",
    "            word_features.append(len(re.findall(r'\\d', word))) # digit feature\n",
    "            word_features.append(len(word)) # count feature\n",
    "        \n",
    "            res.append(word_features)\n",
    "\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, output_dim,\n",
    "                 emb_dim=50, pretrained_emb=None, freeze=False,\n",
    "                num_binary_features=0, num_count_features=0):\n",
    "        \"\"\"\n",
    "        count features will be batch-normalized during forward\n",
    "        \"\"\"\n",
    "        \n",
    "        super(FeedForwardTagger, self).__init__()\n",
    "        \n",
    "        self.num_bin_feat = num_binary_features\n",
    "        self.num_cnt_feat = num_count_features\n",
    "        \n",
    "        if pretrained_emb:\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrain_emb)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "            torch.nn.init.uniform_(self.emb.weight, -0.01, 0.01)\n",
    "        \n",
    "        input_dim = (2 * window_size + 1) * emb_dim\n",
    "        input_dim += self.num_bin_feat + self.num_cnt_feat\n",
    "        \n",
    "        if self.num_cnt_feat != 0:\n",
    "            self.batchnorm = nn.BatchNorm1d(self.num_cnt_feat)\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        input: [emb | binary features | count features]\n",
    "        \"\"\"\n",
    "        num_feat = self.num_bin_feat + self.num_cnt_feat\n",
    "        if num_feat != 0:\n",
    "            to_embed = inputs[:, :-num_feat]\n",
    "            bin_feats = inputs[:, -num_feat : -self.num_cnt_feat]\n",
    "            cnt_feats = inputs[:, -self.num_cnt_feat:]\n",
    "            \n",
    "            # embed up to num_extra_features\n",
    "            embeds = self.emb(to_embed).view((inputs.shape[0], -1))\n",
    "            # normalize count features\n",
    "            cnt_feats = self.batchnorm(cnt_feats.float())\n",
    "            # concat emb w/ extra features\n",
    "            x = torch.cat((embeds, bin_feats, cnt_feats), dim=1)\n",
    "        else:\n",
    "            x = embeds = self.emb(inputs).view((inputs.shape[0], -1))\n",
    "            \n",
    "        out = torch.tanh(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util(model, X_train, Y_train, X_dev, Y_dev, n_epochs, lr, \n",
    "              batch_size):\n",
    "    \"\"\"\n",
    "    returns: best_model, losses, train_accu_list, dev_accu_list\n",
    "    \"\"\"\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_model = None\n",
    "    best_dev_accu = 0\n",
    "    losses = []\n",
    "    train_accu_list, dev_accu_list = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(X_train[i : i + batch_size])\n",
    "            loss = loss_func(log_probs, Y_train[i : i + batch_size])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_preds = torch.argmax(model(X_train), dim=1)\n",
    "        train_accu = accuracy_score(Y_train, train_preds)\n",
    "        # evaluate on dev\n",
    "        dev_preds = torch.argmax(model(X_dev), dim=1)\n",
    "        dev_accu = accuracy_score(Y_dev, dev_preds)\n",
    "        \n",
    "        # early stopping, save the model if it has improved on dev\n",
    "        if dev_accu > best_dev_accu:\n",
    "            best_dev_accu = dev_accu\n",
    "            best_model = deepcopy(model)\n",
    "        \n",
    "        print('Epoch {}: train_loss {:.4f}, train_accu: {:.4f}, dev_accu: {:.4f}'\\\n",
    "              .format(epoch, epoch_loss, train_accu, dev_accu))\n",
    "        losses.append(epoch_loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        dev_accu_list.append(dev_accu)\n",
    "        \n",
    "    loss_accu_df = pd.DataFrame({\n",
    "        'epoch': range(n_epochs), \n",
    "        'loss': losses,\n",
    "        'train_accu': train_accu_list,\n",
    "        'dev_accu': dev_accu_list})\n",
    "        \n",
    "    return best_model, loss_accu_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accu(loss_accu_df_list, window_list):\n",
    "    \"\"\"\n",
    "    input: two lists of the same length, loss_accu_df, window\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for df, w in zip(loss_accu_df_list, window_list):\n",
    "        df1 = df.melt(\n",
    "            'epoch', value_vars=['loss']).assign(window=w, plot='loss')\n",
    "        df2 = df.melt(\n",
    "            'epoch', value_vars=['train_accu', 'dev_accu']).assign(window=w, plot='accu')\n",
    "        dfs.extend([df1, df2])\n",
    "    plot_df = pd.concat(dfs)\n",
    "\n",
    "    g = sns.FacetGrid(data=plot_df, row='plot', col='window', \n",
    "                      hue='variable', sharey=False)\n",
    "    g.map_dataframe(sns.lineplot, x='epoch', y='value')\n",
    "    g.add_legend()\n",
    "\n",
    "def plot_confusion_matrix(matrix, labels, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, \n",
    "                     annot=True, fmt='d', cmap='Blues')\n",
    "    ax.set_xlabel('Predictions')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, train_vocab = read_corpus(DATADIR + 'twpos-train.tsv')\n",
    "dev, dev_labels, dev_vocab = read_corpus(DATADIR + 'twpos-dev.tsv')\n",
    "devtest, devtest_labels, devtest_vocab = read_corpus(DATADIR + 'twpos-devtest.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.unique(train_labels)\n",
    "all_labels_devtest = np.unique(devtest_labels) # devtest labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "Y_train = label_encoder.transform(train_labels)\n",
    "Y_dev = label_encoder.transform(dev_labels)\n",
    "Y_devtest = label_encoder.transform(devtest_labels)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_dev = torch.tensor(Y_dev, dtype=torch.long)\n",
    "Y_devtest = torch.tensor(Y_devtest, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devtest labels\n",
    "all_labels_devtest = np.unique(devtest_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_vocab.copy()\n",
    "vocab.update(dev_vocab)\n",
    "vocab.update(devtest_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline w/ Randomly Initialized Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for randomly initialized embs\n",
    "idx2word_rand = sorted(vocab)\n",
    "idx2word_rand += ['<s>', '</s>', 'UUUNKKK']\n",
    "word2idx_rand = {word: idx for idx, word in enumerate(idx2word_rand)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, DevTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0 = encode_lines(train, word2idx_rand, window_size=0)\n",
    "X_dev_w0 = encode_lines(dev, word2idx_rand, window_size=0)\n",
    "X_devtest_w0 = encode_lines(devtest, word2idx_rand, window_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 1\n",
    "X_train_w1 = encode_lines(train, word2idx_rand, window_size=1)\n",
    "X_dev_w1 = encode_lines(dev, word2idx_rand, window_size=1)\n",
    "X_devtest_w1 = encode_lines(devtest, word2idx_rand, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_w0 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=0,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w0, df_w0 = train_util(model_w0, X_train_w0, Y_train, X_dev_w0, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on devtest\n",
    "devtest_preds = torch.argmax(best_model_w0(X_devtest_w0), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w1 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=1,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w1, df_w1 = train_util(model_w1, X_train_w1, Y_train, X_dev_w1, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1(X_devtest_w1), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses and accuracy, and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accu([df_w0, df_w1], window_list=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_w0, all_labels_devtest, 'w=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_w0, all_labels_devtest, 'w=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n",
    "In addition to the following binary features, I also added a count feature for digits and a count feature for the length of the entire token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['#', '%', \"'\", '/', ':', 'â€™']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = construct_features(train, FEATURES)\n",
    "X_dev_features = construct_features(dev, FEATURES)\n",
    "X_devtest_features = construct_features(devtest, FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w0_feat = torch.cat((X_train_w0, X_train_features), dim=1)\n",
    "X_dev_w0_feat = torch.cat((X_dev_w0, X_dev_features), dim=1)\n",
    "X_devtest_w0_feat = torch.cat((X_devtest_w0, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w1_feat = torch.cat((X_train_w1, X_train_features), dim=1)\n",
    "X_dev_w1_feat = torch.cat((X_dev_w1, X_dev_features), dim=1)\n",
    "X_devtest_w1_feat = torch.cat((X_devtest_w1, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 44.4099, train_accu: 0.2244, dev_accu: 0.2358\n",
      "Epoch 1: train_loss 39.6456, train_accu: 0.3888, dev_accu: 0.3804\n",
      "Epoch 2: train_loss 29.9395, train_accu: 0.5029, dev_accu: 0.5005\n",
      "Epoch 3: train_loss 24.2968, train_accu: 0.5964, dev_accu: 0.5878\n",
      "Epoch 4: train_loss 20.8838, train_accu: 0.6673, dev_accu: 0.6636\n",
      "Epoch 5: train_loss 18.6234, train_accu: 0.6963, dev_accu: 0.6816\n",
      "Epoch 6: train_loss 15.9197, train_accu: 0.7255, dev_accu: 0.7071\n",
      "Epoch 7: train_loss 13.9421, train_accu: 0.7529, dev_accu: 0.7308\n",
      "Epoch 8: train_loss 12.3033, train_accu: 0.8041, dev_accu: 0.7525\n",
      "Epoch 9: train_loss 10.8349, train_accu: 0.8329, dev_accu: 0.7635\n",
      "Epoch 10: train_loss 9.4517, train_accu: 0.8639, dev_accu: 0.7693\n",
      "Epoch 11: train_loss 8.4453, train_accu: 0.8741, dev_accu: 0.7714\n",
      "Epoch 12: train_loss 7.7078, train_accu: 0.8858, dev_accu: 0.7789\n",
      "Epoch 13: train_loss 6.8678, train_accu: 0.8987, dev_accu: 0.7864\n",
      "Epoch 14: train_loss 6.2967, train_accu: 0.9034, dev_accu: 0.7861\n",
      "Epoch 15: train_loss 5.8778, train_accu: 0.9086, dev_accu: 0.7855\n",
      "Epoch 16: train_loss 5.5369, train_accu: 0.9114, dev_accu: 0.7882\n",
      "Epoch 17: train_loss 5.2782, train_accu: 0.9137, dev_accu: 0.7899\n",
      "Epoch 18: train_loss 5.0617, train_accu: 0.9144, dev_accu: 0.7907\n",
      "Epoch 19: train_loss 4.8953, train_accu: 0.9154, dev_accu: 0.7917\n",
      "Epoch 20: train_loss 4.7401, train_accu: 0.9168, dev_accu: 0.7920\n",
      "Epoch 21: train_loss 4.6185, train_accu: 0.9183, dev_accu: 0.7917\n",
      "Epoch 22: train_loss 4.5120, train_accu: 0.9187, dev_accu: 0.7938\n",
      "Epoch 23: train_loss 4.4225, train_accu: 0.9193, dev_accu: 0.7928\n",
      "Epoch 24: train_loss 4.3369, train_accu: 0.9196, dev_accu: 0.7936\n"
     ]
    }
   ],
   "source": [
    "model_w0_feat = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                                  window_size=0, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w0_feat, df_w0_feat = \\\n",
    "train_util(model_w0_feat, X_train_w0_feat, Y_train, X_dev_w0_feat, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8021\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w0_feat(X_devtest_w0_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 45.1374, train_accu: 0.1795, dev_accu: 0.1854\n",
      "Epoch 1: train_loss 39.4513, train_accu: 0.2642, dev_accu: 0.2539\n",
      "Epoch 2: train_loss 34.2350, train_accu: 0.3862, dev_accu: 0.3748\n",
      "Epoch 3: train_loss 25.4433, train_accu: 0.6241, dev_accu: 0.6169\n",
      "Epoch 4: train_loss 19.7850, train_accu: 0.6755, dev_accu: 0.6677\n",
      "Epoch 5: train_loss 16.8015, train_accu: 0.7195, dev_accu: 0.7019\n",
      "Epoch 6: train_loss 14.6386, train_accu: 0.7490, dev_accu: 0.7283\n",
      "Epoch 7: train_loss 12.7468, train_accu: 0.7805, dev_accu: 0.7484\n",
      "Epoch 8: train_loss 11.1426, train_accu: 0.8099, dev_accu: 0.7650\n",
      "Epoch 9: train_loss 9.8024, train_accu: 0.8365, dev_accu: 0.7772\n",
      "Epoch 10: train_loss 8.6743, train_accu: 0.8507, dev_accu: 0.7818\n",
      "Epoch 11: train_loss 7.5694, train_accu: 0.8794, dev_accu: 0.7963\n",
      "Epoch 12: train_loss 6.6847, train_accu: 0.8977, dev_accu: 0.8023\n",
      "Epoch 13: train_loss 5.9266, train_accu: 0.9105, dev_accu: 0.8108\n",
      "Epoch 14: train_loss 5.2365, train_accu: 0.9197, dev_accu: 0.8146\n",
      "Epoch 15: train_loss 4.7002, train_accu: 0.9298, dev_accu: 0.8214\n",
      "Epoch 16: train_loss 4.2559, train_accu: 0.9352, dev_accu: 0.8241\n",
      "Epoch 17: train_loss 3.8744, train_accu: 0.9408, dev_accu: 0.8283\n",
      "Epoch 18: train_loss 3.5379, train_accu: 0.9445, dev_accu: 0.8295\n",
      "Epoch 19: train_loss 3.2419, train_accu: 0.9479, dev_accu: 0.8307\n",
      "Epoch 20: train_loss 2.9827, train_accu: 0.9504, dev_accu: 0.8314\n",
      "Epoch 21: train_loss 2.7522, train_accu: 0.9539, dev_accu: 0.8305\n",
      "Epoch 22: train_loss 2.5449, train_accu: 0.9586, dev_accu: 0.8318\n",
      "Epoch 23: train_loss 2.3571, train_accu: 0.9637, dev_accu: 0.8336\n",
      "Epoch 24: train_loss 2.1858, train_accu: 0.9693, dev_accu: 0.8368\n"
     ]
    }
   ],
   "source": [
    "model_w1_feat = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                                  window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w1_feat, df_w1_feat = \\\n",
    "train_util(model_w1_feat, X_train_w1_feat, Y_train, X_dev_w1_feat, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8441\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1_feat(X_devtest_w1_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for pretrained word embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Architecture Engineering\n",
    "## w = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 2\n",
    "X_train_w2 = encode_lines(train, word2idx_rand, window_size=2)\n",
    "X_dev_w2 = encode_lines(dev, word2idx_rand, window_size=2)\n",
    "X_devtest_w2 = encode_lines(devtest, word2idx_rand, window_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
