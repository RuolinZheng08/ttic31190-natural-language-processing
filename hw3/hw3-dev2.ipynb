{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe331513930>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data/'\n",
    "EMB_DIM = 50 # embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        lines: [['hello', 'world'], ...]\n",
    "        labels: [[!], [N], ...]\n",
    "        vocab\n",
    "    \"\"\"\n",
    "    with open(file, 'rt') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n\\n')\n",
    "    ret_lines = []\n",
    "    labels = []\n",
    "    vocab = set()\n",
    "    for line in lines:\n",
    "        if not line: \n",
    "            continue\n",
    "        curr_line = []\n",
    "        for token_label_str in line.split('\\n'):\n",
    "            if not token_label_str: \n",
    "                continue\n",
    "            token, label = token_label_str.split('\\t')\n",
    "            vocab.add(token)\n",
    "            labels.append(label)\n",
    "            curr_line.append(token)\n",
    "        ret_lines.append(curr_line)\n",
    "    return ret_lines, labels, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lines(lines, word2idx_map, window_size):\n",
    "    \"\"\"\n",
    "    returns X: len(lines) x (2 * window_size + 1)\n",
    "    \"\"\"\n",
    "    def encode_line(line, word2idx_map, window_size):\n",
    "        num_repr = [] # numerical representation\n",
    "        for word in line:\n",
    "            num = word2idx_map.get(word, word2idx_map['UUUNKKK'])\n",
    "            num_repr.append(num)\n",
    "        # pad with start and end tokens\n",
    "        start = [word2idx_map['<s>']] * window_size\n",
    "        end = [word2idx_map['</s>']] * window_size\n",
    "        padded = start + num_repr + end\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(window_size, len(padded) - window_size):\n",
    "            windowed = padded[i - window_size : i + window_size + 1]\n",
    "            ret.append(windowed)\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    res = []\n",
    "    for line in lines:\n",
    "        res.extend(encode_line(line, word2idx_map, window_size))\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(lines, features):\n",
    "    \"\"\"\n",
    "    features: a set of characters to look for in each token\n",
    "    also append to the end:\n",
    "        (count features are normalized during training)\n",
    "        - a count feature for digits\n",
    "        - a count feature for the length of the entire token\n",
    "    returns X: len(lines) x num_features\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            word_features = []\n",
    "            char_set = set(word)\n",
    "            for feature in features:\n",
    "                val = 1 if feature in char_set else 0\n",
    "                word_features.append(val)\n",
    "\n",
    "            word_features.append(len(re.findall(r'\\d', word))) # digit feature\n",
    "            word_features.append(len(word)) # count feature\n",
    "        \n",
    "            res.append(word_features)\n",
    "\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, window_size, output_dim, emb_dim=EMB_DIM, \n",
    "                 vocab_size=None, pretrained_emb=None, freeze=False,\n",
    "                num_binary_features=0, num_count_features=0):\n",
    "        \"\"\"\n",
    "        vocab_size is None when using pretrained emb\n",
    "        count features will be batch-normalized during forward\n",
    "        \"\"\"\n",
    "        \n",
    "        super(FeedForwardTagger, self).__init__()\n",
    "        \n",
    "        self.num_bin_feat = num_binary_features\n",
    "        self.num_cnt_feat = num_count_features\n",
    "        \n",
    "        if pretrained_emb is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "            torch.nn.init.uniform_(self.emb.weight, -0.01, 0.01)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrained_emb)\n",
    "        \n",
    "        input_dim = (2 * window_size + 1) * emb_dim\n",
    "        input_dim += self.num_bin_feat + self.num_cnt_feat\n",
    "        \n",
    "        if self.num_cnt_feat != 0:\n",
    "            self.batchnorm = nn.BatchNorm1d(self.num_cnt_feat)\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        input: [emb | binary features | count features]\n",
    "        \"\"\"\n",
    "        num_feat = self.num_bin_feat + self.num_cnt_feat\n",
    "        if num_feat != 0:\n",
    "            to_embed = inputs[:, :-num_feat]\n",
    "            bin_feats = inputs[:, -num_feat : -self.num_cnt_feat]\n",
    "            cnt_feats = inputs[:, -self.num_cnt_feat:]\n",
    "            \n",
    "            # embed up to num_extra_features\n",
    "            embeds = self.emb(to_embed).view((inputs.shape[0], -1))\n",
    "            # normalize count features\n",
    "            cnt_feats = self.batchnorm(cnt_feats.float())\n",
    "            # concat emb w/ extra features\n",
    "            x = torch.cat((embeds, bin_feats, cnt_feats), dim=1)\n",
    "        else:\n",
    "            x = self.emb(inputs).view((inputs.shape[0], -1))\n",
    "            \n",
    "        out = torch.tanh(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util(model, X_train, Y_train, X_dev, Y_dev, n_epochs, lr, \n",
    "              batch_size):\n",
    "    \"\"\"\n",
    "    returns: best_model, losses, train_accu_list, dev_accu_list\n",
    "    \"\"\"\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_model = None\n",
    "    best_dev_accu = 0\n",
    "    losses = []\n",
    "    train_accu_list, dev_accu_list = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(X_train[i : i + batch_size])\n",
    "            loss = loss_func(log_probs, Y_train[i : i + batch_size])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_preds = torch.argmax(model(X_train), dim=1)\n",
    "        train_accu = accuracy_score(Y_train, train_preds)\n",
    "        # evaluate on dev\n",
    "        dev_preds = torch.argmax(model(X_dev), dim=1)\n",
    "        dev_accu = accuracy_score(Y_dev, dev_preds)\n",
    "        \n",
    "        # early stopping, save the model if it has improved on dev\n",
    "        if dev_accu > best_dev_accu:\n",
    "            best_dev_accu = dev_accu\n",
    "            best_model = deepcopy(model)\n",
    "        \n",
    "        print('Epoch {}: train_loss {:.4f}, train_accu: {:.4f}, dev_accu: {:.4f}'\\\n",
    "              .format(epoch, epoch_loss, train_accu, dev_accu))\n",
    "        losses.append(epoch_loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        dev_accu_list.append(dev_accu)\n",
    "        \n",
    "    loss_accu_df = pd.DataFrame({\n",
    "        'epoch': range(n_epochs), \n",
    "        'loss': losses,\n",
    "        'train_accu': train_accu_list,\n",
    "        'dev_accu': dev_accu_list})\n",
    "        \n",
    "    return best_model, loss_accu_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accu(loss_accu_df_list, window_list):\n",
    "    \"\"\"\n",
    "    input: two lists of the same length, loss_accu_df, window\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for df, w in zip(loss_accu_df_list, window_list):\n",
    "        df1 = df.melt(\n",
    "            'epoch', value_vars=['loss']).assign(window=w, plot='loss')\n",
    "        df2 = df.melt(\n",
    "            'epoch', value_vars=['train_accu', 'dev_accu']).assign(window=w, plot='accu')\n",
    "        dfs.extend([df1, df2])\n",
    "    plot_df = pd.concat(dfs)\n",
    "\n",
    "    g = sns.FacetGrid(data=plot_df, row='plot', col='window', \n",
    "                      hue='variable', sharey=False)\n",
    "    g.map_dataframe(sns.lineplot, x='epoch', y='value')\n",
    "    g.add_legend()\n",
    "\n",
    "def plot_confusion_matrix(matrix, labels, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, \n",
    "                     annot=True, fmt='d', cmap='Blues')\n",
    "    ax.set_xlabel('Predictions')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, train_vocab = read_corpus(DATADIR + 'twpos-train.tsv')\n",
    "dev, dev_labels, dev_vocab = read_corpus(DATADIR + 'twpos-dev.tsv')\n",
    "devtest, devtest_labels, devtest_vocab = read_corpus(DATADIR + 'twpos-devtest.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.unique(train_labels)\n",
    "all_labels_devtest = np.unique(devtest_labels) # devtest labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "Y_train = label_encoder.transform(train_labels)\n",
    "Y_dev = label_encoder.transform(dev_labels)\n",
    "Y_devtest = label_encoder.transform(devtest_labels)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_dev = torch.tensor(Y_dev, dtype=torch.long)\n",
    "Y_devtest = torch.tensor(Y_devtest, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devtest labels\n",
    "all_labels_devtest = np.unique(devtest_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_vocab.copy()\n",
    "vocab.update(dev_vocab)\n",
    "vocab.update(devtest_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline w/ Randomly Initialized Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for randomly initialized embs\n",
    "idx2word_rand = sorted(vocab)\n",
    "idx2word_rand += ['<s>', '</s>', 'UUUNKKK']\n",
    "word2idx_rand = {word: idx for idx, word in enumerate(idx2word_rand)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, DevTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0 = encode_lines(train, word2idx_rand, window_size=0)\n",
    "X_dev_w0 = encode_lines(dev, word2idx_rand, window_size=0)\n",
    "X_devtest_w0 = encode_lines(devtest, word2idx_rand, window_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 1\n",
    "X_train_w1 = encode_lines(train, word2idx_rand, window_size=1)\n",
    "X_dev_w1 = encode_lines(dev, word2idx_rand, window_size=1)\n",
    "X_devtest_w1 = encode_lines(devtest, word2idx_rand, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 25.5599, train_accu: 0.1514, dev_accu: 0.1558\n",
      "Epoch 1: train_loss 23.7730, train_accu: 0.2286, dev_accu: 0.2340\n",
      "Epoch 2: train_loss 22.1405, train_accu: 0.2680, dev_accu: 0.2690\n",
      "Epoch 3: train_loss 20.1998, train_accu: 0.3710, dev_accu: 0.3727\n",
      "Epoch 4: train_loss 17.8418, train_accu: 0.4738, dev_accu: 0.4723\n",
      "Epoch 5: train_loss 15.4302, train_accu: 0.4986, dev_accu: 0.4960\n",
      "Epoch 6: train_loss 14.1085, train_accu: 0.5089, dev_accu: 0.5065\n",
      "Epoch 7: train_loss 13.2731, train_accu: 0.5258, dev_accu: 0.5248\n",
      "Epoch 8: train_loss 12.4983, train_accu: 0.5381, dev_accu: 0.5403\n",
      "Epoch 9: train_loss 11.7002, train_accu: 0.6463, dev_accu: 0.6304\n",
      "Epoch 10: train_loss 11.0439, train_accu: 0.5687, dev_accu: 0.5671\n",
      "Epoch 11: train_loss 10.4354, train_accu: 0.6447, dev_accu: 0.6320\n",
      "Epoch 12: train_loss 9.8166, train_accu: 0.7130, dev_accu: 0.6743\n",
      "Epoch 13: train_loss 8.9609, train_accu: 0.6921, dev_accu: 0.6737\n",
      "Epoch 14: train_loss 8.3989, train_accu: 0.7376, dev_accu: 0.6988\n",
      "Epoch 15: train_loss 7.7715, train_accu: 0.7280, dev_accu: 0.6984\n",
      "Epoch 16: train_loss 7.2619, train_accu: 0.7608, dev_accu: 0.7127\n",
      "Epoch 17: train_loss 6.7517, train_accu: 0.7679, dev_accu: 0.7187\n",
      "Epoch 18: train_loss 6.2951, train_accu: 0.7825, dev_accu: 0.7293\n",
      "Epoch 19: train_loss 5.8800, train_accu: 0.7897, dev_accu: 0.7359\n",
      "Epoch 20: train_loss 5.5177, train_accu: 0.8097, dev_accu: 0.7449\n",
      "Epoch 21: train_loss 5.2040, train_accu: 0.8238, dev_accu: 0.7480\n",
      "Epoch 22: train_loss 4.9066, train_accu: 0.8543, dev_accu: 0.7540\n",
      "Epoch 23: train_loss 4.6479, train_accu: 0.8622, dev_accu: 0.7548\n",
      "Epoch 24: train_loss 4.3913, train_accu: 0.8684, dev_accu: 0.7521\n"
     ]
    }
   ],
   "source": [
    "model_w0 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=0,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w0, df_w0 = train_util(model_w0, X_train_w0, Y_train, X_dev_w0, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on devtest\n",
    "devtest_preds = torch.argmax(best_model_w0(X_devtest_w0), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w1 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=1,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w1, df_w1 = train_util(model_w1, X_train_w1, Y_train, X_dev_w1, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1(X_devtest_w1), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses and accuracy, and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accu([df_w0, df_w1], window_list=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_w0, all_labels_devtest, 'w=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_w0, all_labels_devtest, 'w=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n",
    "In addition to the following binary features, I also added a count feature for digits and a count feature for the length of the entire token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['#', '%', \"'\", '/', ':', '’']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = construct_features(train, FEATURES)\n",
    "X_dev_features = construct_features(dev, FEATURES)\n",
    "X_devtest_features = construct_features(devtest, FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w0_feat = torch.cat((X_train_w0, X_train_features), dim=1)\n",
    "X_dev_w0_feat = torch.cat((X_dev_w0, X_dev_features), dim=1)\n",
    "X_devtest_w0_feat = torch.cat((X_devtest_w0, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w1_feat = torch.cat((X_train_w1, X_train_features), dim=1)\n",
    "X_dev_w1_feat = torch.cat((X_dev_w1, X_dev_features), dim=1)\n",
    "X_devtest_w1_feat = torch.cat((X_devtest_w1, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 44.4099, train_accu: 0.2244, dev_accu: 0.2358\n",
      "Epoch 1: train_loss 39.6456, train_accu: 0.3888, dev_accu: 0.3804\n",
      "Epoch 2: train_loss 29.9395, train_accu: 0.5029, dev_accu: 0.5005\n",
      "Epoch 3: train_loss 24.2968, train_accu: 0.5964, dev_accu: 0.5878\n",
      "Epoch 4: train_loss 20.8838, train_accu: 0.6673, dev_accu: 0.6636\n",
      "Epoch 5: train_loss 18.6234, train_accu: 0.6963, dev_accu: 0.6816\n",
      "Epoch 6: train_loss 15.9197, train_accu: 0.7255, dev_accu: 0.7071\n",
      "Epoch 7: train_loss 13.9421, train_accu: 0.7529, dev_accu: 0.7308\n",
      "Epoch 8: train_loss 12.3033, train_accu: 0.8041, dev_accu: 0.7525\n",
      "Epoch 9: train_loss 10.8349, train_accu: 0.8329, dev_accu: 0.7635\n",
      "Epoch 10: train_loss 9.4517, train_accu: 0.8639, dev_accu: 0.7693\n",
      "Epoch 11: train_loss 8.4453, train_accu: 0.8741, dev_accu: 0.7714\n",
      "Epoch 12: train_loss 7.7078, train_accu: 0.8858, dev_accu: 0.7789\n",
      "Epoch 13: train_loss 6.8678, train_accu: 0.8987, dev_accu: 0.7864\n",
      "Epoch 14: train_loss 6.2967, train_accu: 0.9034, dev_accu: 0.7861\n",
      "Epoch 15: train_loss 5.8778, train_accu: 0.9086, dev_accu: 0.7855\n",
      "Epoch 16: train_loss 5.5369, train_accu: 0.9114, dev_accu: 0.7882\n",
      "Epoch 17: train_loss 5.2782, train_accu: 0.9137, dev_accu: 0.7899\n",
      "Epoch 18: train_loss 5.0617, train_accu: 0.9144, dev_accu: 0.7907\n",
      "Epoch 19: train_loss 4.8953, train_accu: 0.9154, dev_accu: 0.7917\n",
      "Epoch 20: train_loss 4.7401, train_accu: 0.9168, dev_accu: 0.7920\n",
      "Epoch 21: train_loss 4.6185, train_accu: 0.9183, dev_accu: 0.7917\n",
      "Epoch 22: train_loss 4.5120, train_accu: 0.9187, dev_accu: 0.7938\n",
      "Epoch 23: train_loss 4.4225, train_accu: 0.9193, dev_accu: 0.7928\n",
      "Epoch 24: train_loss 4.3369, train_accu: 0.9196, dev_accu: 0.7936\n"
     ]
    }
   ],
   "source": [
    "model_w0_feat = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                                  window_size=0, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w0_feat, df_w0_feat = \\\n",
    "train_util(model_w0_feat, X_train_w0_feat, Y_train, X_dev_w0_feat, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8021\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w0_feat(X_devtest_w0_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 45.1374, train_accu: 0.1795, dev_accu: 0.1854\n",
      "Epoch 1: train_loss 39.4513, train_accu: 0.2642, dev_accu: 0.2539\n",
      "Epoch 2: train_loss 34.2350, train_accu: 0.3862, dev_accu: 0.3748\n",
      "Epoch 3: train_loss 25.4433, train_accu: 0.6241, dev_accu: 0.6169\n",
      "Epoch 4: train_loss 19.7850, train_accu: 0.6755, dev_accu: 0.6677\n",
      "Epoch 5: train_loss 16.8015, train_accu: 0.7195, dev_accu: 0.7019\n",
      "Epoch 6: train_loss 14.6386, train_accu: 0.7490, dev_accu: 0.7283\n",
      "Epoch 7: train_loss 12.7468, train_accu: 0.7805, dev_accu: 0.7484\n",
      "Epoch 8: train_loss 11.1426, train_accu: 0.8099, dev_accu: 0.7650\n",
      "Epoch 9: train_loss 9.8024, train_accu: 0.8365, dev_accu: 0.7772\n",
      "Epoch 10: train_loss 8.6743, train_accu: 0.8507, dev_accu: 0.7818\n",
      "Epoch 11: train_loss 7.5694, train_accu: 0.8794, dev_accu: 0.7963\n",
      "Epoch 12: train_loss 6.6847, train_accu: 0.8977, dev_accu: 0.8023\n",
      "Epoch 13: train_loss 5.9266, train_accu: 0.9105, dev_accu: 0.8108\n",
      "Epoch 14: train_loss 5.2365, train_accu: 0.9197, dev_accu: 0.8146\n",
      "Epoch 15: train_loss 4.7002, train_accu: 0.9298, dev_accu: 0.8214\n",
      "Epoch 16: train_loss 4.2559, train_accu: 0.9352, dev_accu: 0.8241\n",
      "Epoch 17: train_loss 3.8744, train_accu: 0.9408, dev_accu: 0.8283\n",
      "Epoch 18: train_loss 3.5379, train_accu: 0.9445, dev_accu: 0.8295\n",
      "Epoch 19: train_loss 3.2419, train_accu: 0.9479, dev_accu: 0.8307\n",
      "Epoch 20: train_loss 2.9827, train_accu: 0.9504, dev_accu: 0.8314\n",
      "Epoch 21: train_loss 2.7522, train_accu: 0.9539, dev_accu: 0.8305\n",
      "Epoch 22: train_loss 2.5449, train_accu: 0.9586, dev_accu: 0.8318\n",
      "Epoch 23: train_loss 2.3571, train_accu: 0.9637, dev_accu: 0.8336\n",
      "Epoch 24: train_loss 2.1858, train_accu: 0.9693, dev_accu: 0.8368\n"
     ]
    }
   ],
   "source": [
    "model_w1_feat = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                                  window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w1_feat, df_w1_feat = \\\n",
    "train_util(model_w1_feat, X_train_w1_feat, Y_train, X_dev_w1_feat, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8441\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1_feat(X_devtest_w1_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.read_csv(DATADIR + 'twitter-embeddings.txt', \n",
    "                     delimiter=' ', header=None)\n",
    "twitter_vocab = emb_df[0].to_list()\n",
    "twitter_emb = torch.FloatTensor(emb_df.drop(columns=0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add random emb [-0.01, 0.01] for <s>\n",
    "idx2word_pretrained = twitter_vocab + ['<s>']\n",
    "temp = torch.FloatTensor(1, EMB_DIM).uniform_(-0.01, 0.01)\n",
    "# construct maps for pretrained word embs\n",
    "twitter_emb = torch.cat((twitter_emb, temp))\n",
    "word2idx_pretrained = {word: idx for idx, word in enumerate(idx2word_pretrained)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, Devtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0_pre = encode_lines(train, word2idx_pretrained, window_size=0)\n",
    "X_dev_w0_pre = encode_lines(dev, word2idx_pretrained, window_size=0)\n",
    "X_devtest_w0_pre = encode_lines(devtest, word2idx_pretrained, window_size=0)\n",
    "\n",
    "# w = 1\n",
    "X_train_w1_pre = encode_lines(train, word2idx_pretrained, window_size=1)\n",
    "X_dev_w1_pre = encode_lines(dev, word2idx_pretrained, window_size=1)\n",
    "X_devtest_w1_pre = encode_lines(devtest, word2idx_rand, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 41.9021, train_accu: 0.3067, dev_accu: 0.3006\n",
      "Epoch 1: train_loss 34.8255, train_accu: 0.3086, dev_accu: 0.3026\n",
      "Epoch 2: train_loss 33.7514, train_accu: 0.3208, dev_accu: 0.3157\n",
      "Epoch 3: train_loss 33.2766, train_accu: 0.3247, dev_accu: 0.3178\n",
      "Epoch 4: train_loss 32.9944, train_accu: 0.3277, dev_accu: 0.3201\n",
      "Epoch 5: train_loss 32.7913, train_accu: 0.3292, dev_accu: 0.3221\n",
      "Epoch 6: train_loss 32.6314, train_accu: 0.3392, dev_accu: 0.3331\n",
      "Epoch 7: train_loss 32.4998, train_accu: 0.3401, dev_accu: 0.3335\n",
      "Epoch 8: train_loss 32.3877, train_accu: 0.4435, dev_accu: 0.4449\n",
      "Epoch 9: train_loss 32.2923, train_accu: 0.4442, dev_accu: 0.4449\n",
      "Epoch 10: train_loss 32.2179, train_accu: 0.4447, dev_accu: 0.4460\n",
      "Epoch 11: train_loss 32.1766, train_accu: 0.4451, dev_accu: 0.4460\n",
      "Epoch 12: train_loss 32.1069, train_accu: 0.4331, dev_accu: 0.4360\n",
      "Epoch 13: train_loss 32.3136, train_accu: 0.4332, dev_accu: 0.4360\n",
      "Epoch 14: train_loss 31.9996, train_accu: 0.4459, dev_accu: 0.4468\n",
      "Epoch 15: train_loss 32.0168, train_accu: 0.4468, dev_accu: 0.4480\n",
      "Epoch 16: train_loss 31.9089, train_accu: 0.4471, dev_accu: 0.4480\n",
      "Epoch 17: train_loss 31.8725, train_accu: 0.4474, dev_accu: 0.4482\n",
      "Epoch 18: train_loss 31.8394, train_accu: 0.4476, dev_accu: 0.4487\n",
      "Epoch 19: train_loss 31.8148, train_accu: 0.4482, dev_accu: 0.4495\n",
      "Epoch 20: train_loss 31.8021, train_accu: 0.4483, dev_accu: 0.4509\n",
      "Epoch 21: train_loss 31.7724, train_accu: 0.4490, dev_accu: 0.4509\n",
      "Epoch 22: train_loss 31.9130, train_accu: 0.4361, dev_accu: 0.4404\n",
      "Epoch 23: train_loss 31.7339, train_accu: 0.4491, dev_accu: 0.4520\n",
      "Epoch 24: train_loss 31.7343, train_accu: 0.4489, dev_accu: 0.4516\n",
      "Epoch 25: train_loss 31.7315, train_accu: 0.4496, dev_accu: 0.4524\n",
      "Epoch 26: train_loss 31.6729, train_accu: 0.4498, dev_accu: 0.4524\n",
      "Epoch 27: train_loss 31.6529, train_accu: 0.4501, dev_accu: 0.4530\n",
      "Epoch 28: train_loss 31.6335, train_accu: 0.4501, dev_accu: 0.4534\n",
      "Epoch 29: train_loss 31.6215, train_accu: 0.4503, dev_accu: 0.4536\n",
      "Epoch 30: train_loss 31.6087, train_accu: 0.4505, dev_accu: 0.4534\n",
      "Epoch 31: train_loss 31.5929, train_accu: 0.4507, dev_accu: 0.4532\n",
      "Epoch 32: train_loss 31.5964, train_accu: 0.4377, dev_accu: 0.4422\n",
      "Epoch 33: train_loss 31.6009, train_accu: 0.4378, dev_accu: 0.4422\n",
      "Epoch 34: train_loss 31.5440, train_accu: 0.4380, dev_accu: 0.4424\n",
      "Epoch 35: train_loss 31.6062, train_accu: 0.3538, dev_accu: 0.3555\n",
      "Epoch 36: train_loss 31.6191, train_accu: 0.4512, dev_accu: 0.4541\n",
      "Epoch 37: train_loss 31.5379, train_accu: 0.4513, dev_accu: 0.4541\n",
      "Epoch 38: train_loss 31.5262, train_accu: 0.4513, dev_accu: 0.4541\n",
      "Epoch 39: train_loss 31.5103, train_accu: 0.4515, dev_accu: 0.4541\n",
      "Epoch 40: train_loss 31.4982, train_accu: 0.4517, dev_accu: 0.4541\n",
      "Epoch 41: train_loss 31.4860, train_accu: 0.4517, dev_accu: 0.4543\n",
      "Epoch 42: train_loss 31.4743, train_accu: 0.4518, dev_accu: 0.4541\n",
      "Epoch 43: train_loss 31.4635, train_accu: 0.4520, dev_accu: 0.4543\n",
      "Epoch 44: train_loss 31.4538, train_accu: 0.4521, dev_accu: 0.4545\n",
      "Epoch 45: train_loss 31.4449, train_accu: 0.4522, dev_accu: 0.4543\n",
      "Epoch 46: train_loss 31.4368, train_accu: 0.4522, dev_accu: 0.4545\n",
      "Epoch 47: train_loss 31.4292, train_accu: 0.4522, dev_accu: 0.4545\n",
      "Epoch 48: train_loss 31.4214, train_accu: 0.4524, dev_accu: 0.4547\n",
      "Epoch 49: train_loss 31.4138, train_accu: 0.4524, dev_accu: 0.4549\n",
      "Epoch 50: train_loss 31.4082, train_accu: 0.4395, dev_accu: 0.4437\n",
      "Epoch 51: train_loss 31.4031, train_accu: 0.4395, dev_accu: 0.4437\n",
      "Epoch 52: train_loss 31.3912, train_accu: 0.4395, dev_accu: 0.4437\n",
      "Epoch 53: train_loss 31.4022, train_accu: 0.4526, dev_accu: 0.4549\n",
      "Epoch 54: train_loss 31.4582, train_accu: 0.4394, dev_accu: 0.4439\n",
      "Epoch 55: train_loss 31.3811, train_accu: 0.4527, dev_accu: 0.4551\n",
      "Epoch 56: train_loss 31.4007, train_accu: 0.4528, dev_accu: 0.4551\n",
      "Epoch 57: train_loss 31.3737, train_accu: 0.4399, dev_accu: 0.4439\n",
      "Epoch 58: train_loss 31.3649, train_accu: 0.4530, dev_accu: 0.4551\n",
      "Epoch 59: train_loss 31.3606, train_accu: 0.4530, dev_accu: 0.4551\n",
      "Epoch 60: train_loss 31.3530, train_accu: 0.4398, dev_accu: 0.4439\n",
      "Epoch 61: train_loss 31.3447, train_accu: 0.4397, dev_accu: 0.4439\n",
      "Epoch 62: train_loss 31.3378, train_accu: 0.4398, dev_accu: 0.4441\n",
      "Epoch 63: train_loss 31.3320, train_accu: 0.4400, dev_accu: 0.4443\n",
      "Epoch 64: train_loss 31.3271, train_accu: 0.4531, dev_accu: 0.4555\n",
      "Epoch 65: train_loss 31.3227, train_accu: 0.4531, dev_accu: 0.4555\n",
      "Epoch 66: train_loss 31.3178, train_accu: 0.4530, dev_accu: 0.4555\n",
      "Epoch 67: train_loss 31.3118, train_accu: 0.4400, dev_accu: 0.4443\n",
      "Epoch 68: train_loss 31.3065, train_accu: 0.4400, dev_accu: 0.4443\n",
      "Epoch 69: train_loss 31.3034, train_accu: 0.4401, dev_accu: 0.4443\n",
      "Epoch 70: train_loss 31.3010, train_accu: 0.4403, dev_accu: 0.4441\n",
      "Epoch 71: train_loss 31.3012, train_accu: 0.4403, dev_accu: 0.4441\n",
      "Epoch 72: train_loss 31.3211, train_accu: 0.4404, dev_accu: 0.4439\n",
      "Epoch 73: train_loss 31.3205, train_accu: 0.4405, dev_accu: 0.4443\n",
      "Epoch 74: train_loss 31.2858, train_accu: 0.4534, dev_accu: 0.4553\n",
      "Epoch 75: train_loss 31.3699, train_accu: 0.4539, dev_accu: 0.4553\n",
      "Epoch 76: train_loss 31.2934, train_accu: 0.4535, dev_accu: 0.4553\n",
      "Epoch 77: train_loss 31.2843, train_accu: 0.4535, dev_accu: 0.4553\n",
      "Epoch 78: train_loss 31.2792, train_accu: 0.4535, dev_accu: 0.4551\n",
      "Epoch 79: train_loss 31.2725, train_accu: 0.4536, dev_accu: 0.4551\n",
      "Epoch 80: train_loss 31.2664, train_accu: 0.4536, dev_accu: 0.4553\n",
      "Epoch 81: train_loss 31.2612, train_accu: 0.4536, dev_accu: 0.4553\n",
      "Epoch 82: train_loss 31.2564, train_accu: 0.4536, dev_accu: 0.4551\n",
      "Epoch 83: train_loss 31.2520, train_accu: 0.4537, dev_accu: 0.4553\n",
      "Epoch 84: train_loss 31.2478, train_accu: 0.4539, dev_accu: 0.4553\n",
      "Epoch 85: train_loss 31.2437, train_accu: 0.4408, dev_accu: 0.4441\n",
      "Epoch 86: train_loss 31.2399, train_accu: 0.4410, dev_accu: 0.4441\n",
      "Epoch 87: train_loss 31.2363, train_accu: 0.4410, dev_accu: 0.4441\n",
      "Epoch 88: train_loss 31.2328, train_accu: 0.4412, dev_accu: 0.4441\n",
      "Epoch 89: train_loss 31.2295, train_accu: 0.4413, dev_accu: 0.4441\n",
      "Epoch 90: train_loss 31.2263, train_accu: 0.4413, dev_accu: 0.4441\n",
      "Epoch 91: train_loss 31.2233, train_accu: 0.4414, dev_accu: 0.4441\n",
      "Epoch 92: train_loss 31.2204, train_accu: 0.4414, dev_accu: 0.4441\n",
      "Epoch 93: train_loss 31.2176, train_accu: 0.4414, dev_accu: 0.4441\n",
      "Epoch 94: train_loss 31.2149, train_accu: 0.4414, dev_accu: 0.4441\n",
      "Epoch 95: train_loss 31.2123, train_accu: 0.4415, dev_accu: 0.4441\n",
      "Epoch 96: train_loss 31.2098, train_accu: 0.4416, dev_accu: 0.4441\n",
      "Epoch 97: train_loss 31.2074, train_accu: 0.4416, dev_accu: 0.4439\n",
      "Epoch 98: train_loss 31.2052, train_accu: 0.4416, dev_accu: 0.4439\n",
      "Epoch 99: train_loss 31.2029, train_accu: 0.4417, dev_accu: 0.4439\n"
     ]
    }
   ],
   "source": [
    "model_w0_tune = FeedForwardTagger(window_size=0, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                  pretrained_emb=twitter_emb)\n",
    "best_model_w0_tune, df_w0_tune = \\\n",
    "train_util(model_w0_tune, X_train_w0_pre, Y_train, X_dev_w0_pre, Y_dev, \n",
    "                                  n_epochs=100, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1_feat(X_devtest_w1_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Architecture Engineering\n",
    "## w = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 2\n",
    "X_train_w2 = encode_lines(train, word2idx_rand, window_size=2)\n",
    "X_dev_w2 = encode_lines(dev, word2idx_rand, window_size=2)\n",
    "X_devtest_w2 = encode_lines(devtest, word2idx_rand, window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
