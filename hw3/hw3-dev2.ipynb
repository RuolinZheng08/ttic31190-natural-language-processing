{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe331513930>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data/'\n",
    "EMB_DIM = 50 # embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        lines: [['hello', 'world'], ...]\n",
    "        labels: [[!], [N], ...]\n",
    "        vocab\n",
    "    \"\"\"\n",
    "    with open(file, 'rt') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n\\n')\n",
    "    ret_lines = []\n",
    "    labels = []\n",
    "    vocab = set()\n",
    "    for line in lines:\n",
    "        if not line: \n",
    "            continue\n",
    "        curr_line = []\n",
    "        for token_label_str in line.split('\\n'):\n",
    "            if not token_label_str: \n",
    "                continue\n",
    "            token, label = token_label_str.split('\\t')\n",
    "            vocab.add(token)\n",
    "            labels.append(label)\n",
    "            curr_line.append(token)\n",
    "        ret_lines.append(curr_line)\n",
    "    return ret_lines, labels, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lines(lines, word2idx_map, window_size):\n",
    "    \"\"\"\n",
    "    returns X: len(lines) x (2 * window_size + 1)\n",
    "    \"\"\"\n",
    "    def encode_line(line, word2idx_map, window_size):\n",
    "        num_repr = [] # numerical representation\n",
    "        for word in line:\n",
    "            num = word2idx_map.get(word, word2idx_map['UUUNKKK'])\n",
    "            num_repr.append(num)\n",
    "        # pad with start and end tokens\n",
    "        start = [word2idx_map['<s>']] * window_size\n",
    "        end = [word2idx_map['</s>']] * window_size\n",
    "        padded = start + num_repr + end\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(window_size, len(padded) - window_size):\n",
    "            windowed = padded[i - window_size : i + window_size + 1]\n",
    "            ret.append(windowed)\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    res = []\n",
    "    for line in lines:\n",
    "        res.extend(encode_line(line, word2idx_map, window_size))\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(lines, features):\n",
    "    \"\"\"\n",
    "    features: a set of characters to look for in each token\n",
    "    also append to the end:\n",
    "        (count features are normalized during training)\n",
    "        - a count feature for digits\n",
    "        - a count feature for the length of the entire token\n",
    "    returns X: len(lines) x num_features\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            word_features = []\n",
    "            char_set = set(word)\n",
    "            for feature in features:\n",
    "                val = 1 if feature in char_set else 0\n",
    "                word_features.append(val)\n",
    "\n",
    "            word_features.append(len(re.findall(r'\\d', word))) # digit feature\n",
    "            word_features.append(len(word)) # count feature\n",
    "        \n",
    "            res.append(word_features)\n",
    "\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, window_size, output_dim, emb_dim=EMB_DIM, \n",
    "                 vocab_size=None, pretrained_emb=None, freeze=False,\n",
    "                num_binary_features=0, num_count_features=0):\n",
    "        \"\"\"\n",
    "        vocab_size is None when using pretrained emb\n",
    "        count features will be batch-normalized during forward\n",
    "        \"\"\"\n",
    "        \n",
    "        super(FeedForwardTagger, self).__init__()\n",
    "        \n",
    "        self.num_bin_feat = num_binary_features\n",
    "        self.num_cnt_feat = num_count_features\n",
    "        \n",
    "        if pretrained_emb is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "            torch.nn.init.uniform_(self.emb.weight, -0.01, 0.01)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrained_emb, freeze=freeze)\n",
    "        \n",
    "        input_dim = (2 * window_size + 1) * emb_dim\n",
    "        input_dim += self.num_bin_feat + self.num_cnt_feat\n",
    "        \n",
    "        if self.num_cnt_feat != 0:\n",
    "            self.batchnorm = nn.BatchNorm1d(self.num_cnt_feat)\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        input: [emb | binary features | count features]\n",
    "        \"\"\"\n",
    "        num_feat = self.num_bin_feat + self.num_cnt_feat\n",
    "        if num_feat != 0:\n",
    "            to_embed = inputs[:, :-num_feat]\n",
    "            bin_feats = inputs[:, -num_feat : -self.num_cnt_feat]\n",
    "            cnt_feats = inputs[:, -self.num_cnt_feat:]\n",
    "            \n",
    "            # embed up to num_extra_features\n",
    "            embeds = self.emb(to_embed).view((inputs.shape[0], -1))\n",
    "            # normalize count features\n",
    "            cnt_feats = self.batchnorm(cnt_feats.float())\n",
    "            # concat emb w/ extra features\n",
    "            x = torch.cat((embeds, bin_feats, cnt_feats), dim=1)\n",
    "        else:\n",
    "            x = self.emb(inputs).view((inputs.shape[0], -1))\n",
    "            \n",
    "        out = torch.tanh(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util(model, X_train, Y_train, X_dev, Y_dev, n_epochs, lr, \n",
    "              batch_size):\n",
    "    \"\"\"\n",
    "    returns: best_model, losses, train_accu_list, dev_accu_list\n",
    "    \"\"\"\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_model = None\n",
    "    best_dev_accu = 0\n",
    "    losses = []\n",
    "    train_accu_list, dev_accu_list = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(X_train[i : i + batch_size])\n",
    "            loss = loss_func(log_probs, Y_train[i : i + batch_size])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_preds = torch.argmax(model(X_train), dim=1)\n",
    "        train_accu = accuracy_score(Y_train, train_preds)\n",
    "        # evaluate on dev\n",
    "        dev_preds = torch.argmax(model(X_dev), dim=1)\n",
    "        dev_accu = accuracy_score(Y_dev, dev_preds)\n",
    "        \n",
    "        # early stopping, save the model if it has improved on dev\n",
    "        if dev_accu > best_dev_accu:\n",
    "            best_dev_accu = dev_accu\n",
    "            best_model = deepcopy(model)\n",
    "        \n",
    "        print('Epoch {}: train_loss {:.4f}, train_accu: {:.4f}, dev_accu: {:.4f}'\\\n",
    "              .format(epoch, epoch_loss, train_accu, dev_accu))\n",
    "        losses.append(epoch_loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        dev_accu_list.append(dev_accu)\n",
    "        \n",
    "    loss_accu_df = pd.DataFrame({\n",
    "        'epoch': range(n_epochs), \n",
    "        'loss': losses,\n",
    "        'train_accu': train_accu_list,\n",
    "        'dev_accu': dev_accu_list})\n",
    "        \n",
    "    return best_model, loss_accu_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accu(loss_accu_df_list, window_list):\n",
    "    \"\"\"\n",
    "    input: two lists of the same length, loss_accu_df, window\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for df, w in zip(loss_accu_df_list, window_list):\n",
    "        df1 = df.melt(\n",
    "            'epoch', value_vars=['loss']).assign(window=w, plot='loss')\n",
    "        df2 = df.melt(\n",
    "            'epoch', value_vars=['train_accu', 'dev_accu']).assign(window=w, plot='accu')\n",
    "        dfs.extend([df1, df2])\n",
    "    plot_df = pd.concat(dfs)\n",
    "\n",
    "    g = sns.FacetGrid(data=plot_df, row='plot', col='window', \n",
    "                      hue='variable', sharey=False)\n",
    "    g.map_dataframe(sns.lineplot, x='epoch', y='value')\n",
    "    g.add_legend()\n",
    "\n",
    "def plot_confusion_matrix(matrix, labels, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, \n",
    "                     annot=True, fmt='d', cmap='Blues')\n",
    "    ax.set_xlabel('Predictions')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, train_vocab = read_corpus(DATADIR + 'twpos-train.tsv')\n",
    "dev, dev_labels, dev_vocab = read_corpus(DATADIR + 'twpos-dev.tsv')\n",
    "devtest, devtest_labels, devtest_vocab = read_corpus(DATADIR + 'twpos-devtest.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.unique(train_labels)\n",
    "all_labels_devtest = np.unique(devtest_labels) # devtest labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "Y_train = label_encoder.transform(train_labels)\n",
    "Y_dev = label_encoder.transform(dev_labels)\n",
    "Y_devtest = label_encoder.transform(devtest_labels)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_dev = torch.tensor(Y_dev, dtype=torch.long)\n",
    "Y_devtest = torch.tensor(Y_devtest, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devtest labels\n",
    "all_labels_devtest = np.unique(devtest_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_vocab.copy()\n",
    "vocab.update(dev_vocab)\n",
    "vocab.update(devtest_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline w/ Randomly Initialized Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for randomly initialized embs\n",
    "idx2word_rand = sorted(vocab)\n",
    "idx2word_rand += ['<s>', '</s>', 'UUUNKKK']\n",
    "word2idx_rand = {word: idx for idx, word in enumerate(idx2word_rand)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, DevTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0 = encode_lines(train, word2idx_rand, window_size=0)\n",
    "X_dev_w0 = encode_lines(dev, word2idx_rand, window_size=0)\n",
    "X_devtest_w0 = encode_lines(devtest, word2idx_rand, window_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 1\n",
    "X_train_w1 = encode_lines(train, word2idx_rand, window_size=1)\n",
    "X_dev_w1 = encode_lines(dev, word2idx_rand, window_size=1)\n",
    "X_devtest_w1 = encode_lines(devtest, word2idx_rand, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 25.5599, train_accu: 0.1514, dev_accu: 0.1558\n",
      "Epoch 1: train_loss 23.7730, train_accu: 0.2286, dev_accu: 0.2340\n",
      "Epoch 2: train_loss 22.1405, train_accu: 0.2680, dev_accu: 0.2690\n",
      "Epoch 3: train_loss 20.1998, train_accu: 0.3710, dev_accu: 0.3727\n",
      "Epoch 4: train_loss 17.8418, train_accu: 0.4738, dev_accu: 0.4723\n",
      "Epoch 5: train_loss 15.4302, train_accu: 0.4986, dev_accu: 0.4960\n",
      "Epoch 6: train_loss 14.1085, train_accu: 0.5089, dev_accu: 0.5065\n",
      "Epoch 7: train_loss 13.2731, train_accu: 0.5258, dev_accu: 0.5248\n",
      "Epoch 8: train_loss 12.4983, train_accu: 0.5381, dev_accu: 0.5403\n",
      "Epoch 9: train_loss 11.7002, train_accu: 0.6463, dev_accu: 0.6304\n",
      "Epoch 10: train_loss 11.0439, train_accu: 0.5687, dev_accu: 0.5671\n",
      "Epoch 11: train_loss 10.4354, train_accu: 0.6447, dev_accu: 0.6320\n",
      "Epoch 12: train_loss 9.8166, train_accu: 0.7130, dev_accu: 0.6743\n",
      "Epoch 13: train_loss 8.9609, train_accu: 0.6921, dev_accu: 0.6737\n",
      "Epoch 14: train_loss 8.3989, train_accu: 0.7376, dev_accu: 0.6988\n",
      "Epoch 15: train_loss 7.7715, train_accu: 0.7280, dev_accu: 0.6984\n",
      "Epoch 16: train_loss 7.2619, train_accu: 0.7608, dev_accu: 0.7127\n",
      "Epoch 17: train_loss 6.7517, train_accu: 0.7679, dev_accu: 0.7187\n",
      "Epoch 18: train_loss 6.2951, train_accu: 0.7825, dev_accu: 0.7293\n",
      "Epoch 19: train_loss 5.8800, train_accu: 0.7897, dev_accu: 0.7359\n",
      "Epoch 20: train_loss 5.5177, train_accu: 0.8097, dev_accu: 0.7449\n",
      "Epoch 21: train_loss 5.2040, train_accu: 0.8238, dev_accu: 0.7480\n",
      "Epoch 22: train_loss 4.9066, train_accu: 0.8543, dev_accu: 0.7540\n",
      "Epoch 23: train_loss 4.6479, train_accu: 0.8622, dev_accu: 0.7548\n",
      "Epoch 24: train_loss 4.3913, train_accu: 0.8684, dev_accu: 0.7521\n"
     ]
    }
   ],
   "source": [
    "model_w0 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=0,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w0, df_w0 = train_util(model_w0, X_train_w0, Y_train, X_dev_w0, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on devtest\n",
    "devtest_preds = torch.argmax(best_model_w0(X_devtest_w0), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w1 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=1,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w1, df_w1 = train_util(model_w1, X_train_w1, Y_train, X_dev_w1, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1(X_devtest_w1), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses and accuracy, and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accu([df_w0, df_w1], window_list=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_w0, all_labels_devtest, 'w=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_w0, all_labels_devtest, 'w=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n",
    "In addition to the following binary features, I also added a count feature for digits and a count feature for the length of the entire token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['#', '%', \"'\", '/', ':', 'â€™']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = construct_features(train, FEATURES)\n",
    "X_dev_features = construct_features(dev, FEATURES)\n",
    "X_devtest_features = construct_features(devtest, FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w0_feat = torch.cat((X_train_w0, X_train_features), dim=1)\n",
    "X_dev_w0_feat = torch.cat((X_dev_w0, X_dev_features), dim=1)\n",
    "X_devtest_w0_feat = torch.cat((X_devtest_w0, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w1_feat = torch.cat((X_train_w1, X_train_features), dim=1)\n",
    "X_dev_w1_feat = torch.cat((X_dev_w1, X_dev_features), dim=1)\n",
    "X_devtest_w1_feat = torch.cat((X_devtest_w1, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 44.4099, train_accu: 0.2244, dev_accu: 0.2358\n",
      "Epoch 1: train_loss 39.6456, train_accu: 0.3888, dev_accu: 0.3804\n",
      "Epoch 2: train_loss 29.9395, train_accu: 0.5029, dev_accu: 0.5005\n",
      "Epoch 3: train_loss 24.2968, train_accu: 0.5964, dev_accu: 0.5878\n",
      "Epoch 4: train_loss 20.8838, train_accu: 0.6673, dev_accu: 0.6636\n",
      "Epoch 5: train_loss 18.6234, train_accu: 0.6963, dev_accu: 0.6816\n",
      "Epoch 6: train_loss 15.9197, train_accu: 0.7255, dev_accu: 0.7071\n",
      "Epoch 7: train_loss 13.9421, train_accu: 0.7529, dev_accu: 0.7308\n",
      "Epoch 8: train_loss 12.3033, train_accu: 0.8041, dev_accu: 0.7525\n",
      "Epoch 9: train_loss 10.8349, train_accu: 0.8329, dev_accu: 0.7635\n",
      "Epoch 10: train_loss 9.4517, train_accu: 0.8639, dev_accu: 0.7693\n",
      "Epoch 11: train_loss 8.4453, train_accu: 0.8741, dev_accu: 0.7714\n",
      "Epoch 12: train_loss 7.7078, train_accu: 0.8858, dev_accu: 0.7789\n",
      "Epoch 13: train_loss 6.8678, train_accu: 0.8987, dev_accu: 0.7864\n",
      "Epoch 14: train_loss 6.2967, train_accu: 0.9034, dev_accu: 0.7861\n",
      "Epoch 15: train_loss 5.8778, train_accu: 0.9086, dev_accu: 0.7855\n",
      "Epoch 16: train_loss 5.5369, train_accu: 0.9114, dev_accu: 0.7882\n",
      "Epoch 17: train_loss 5.2782, train_accu: 0.9137, dev_accu: 0.7899\n",
      "Epoch 18: train_loss 5.0617, train_accu: 0.9144, dev_accu: 0.7907\n",
      "Epoch 19: train_loss 4.8953, train_accu: 0.9154, dev_accu: 0.7917\n",
      "Epoch 20: train_loss 4.7401, train_accu: 0.9168, dev_accu: 0.7920\n",
      "Epoch 21: train_loss 4.6185, train_accu: 0.9183, dev_accu: 0.7917\n",
      "Epoch 22: train_loss 4.5120, train_accu: 0.9187, dev_accu: 0.7938\n",
      "Epoch 23: train_loss 4.4225, train_accu: 0.9193, dev_accu: 0.7928\n",
      "Epoch 24: train_loss 4.3369, train_accu: 0.9196, dev_accu: 0.7936\n"
     ]
    }
   ],
   "source": [
    "model_w0_feat = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                                  window_size=0, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w0_feat, df_w0_feat = \\\n",
    "train_util(model_w0_feat, X_train_w0_feat, Y_train, X_dev_w0_feat, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8021\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w0_feat(X_devtest_w0_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 45.1374, train_accu: 0.1795, dev_accu: 0.1854\n",
      "Epoch 1: train_loss 39.4513, train_accu: 0.2642, dev_accu: 0.2539\n",
      "Epoch 2: train_loss 34.2350, train_accu: 0.3862, dev_accu: 0.3748\n",
      "Epoch 3: train_loss 25.4433, train_accu: 0.6241, dev_accu: 0.6169\n",
      "Epoch 4: train_loss 19.7850, train_accu: 0.6755, dev_accu: 0.6677\n",
      "Epoch 5: train_loss 16.8015, train_accu: 0.7195, dev_accu: 0.7019\n",
      "Epoch 6: train_loss 14.6386, train_accu: 0.7490, dev_accu: 0.7283\n",
      "Epoch 7: train_loss 12.7468, train_accu: 0.7805, dev_accu: 0.7484\n",
      "Epoch 8: train_loss 11.1426, train_accu: 0.8099, dev_accu: 0.7650\n",
      "Epoch 9: train_loss 9.8024, train_accu: 0.8365, dev_accu: 0.7772\n",
      "Epoch 10: train_loss 8.6743, train_accu: 0.8507, dev_accu: 0.7818\n",
      "Epoch 11: train_loss 7.5694, train_accu: 0.8794, dev_accu: 0.7963\n",
      "Epoch 12: train_loss 6.6847, train_accu: 0.8977, dev_accu: 0.8023\n",
      "Epoch 13: train_loss 5.9266, train_accu: 0.9105, dev_accu: 0.8108\n",
      "Epoch 14: train_loss 5.2365, train_accu: 0.9197, dev_accu: 0.8146\n",
      "Epoch 15: train_loss 4.7002, train_accu: 0.9298, dev_accu: 0.8214\n",
      "Epoch 16: train_loss 4.2559, train_accu: 0.9352, dev_accu: 0.8241\n",
      "Epoch 17: train_loss 3.8744, train_accu: 0.9408, dev_accu: 0.8283\n",
      "Epoch 18: train_loss 3.5379, train_accu: 0.9445, dev_accu: 0.8295\n",
      "Epoch 19: train_loss 3.2419, train_accu: 0.9479, dev_accu: 0.8307\n",
      "Epoch 20: train_loss 2.9827, train_accu: 0.9504, dev_accu: 0.8314\n",
      "Epoch 21: train_loss 2.7522, train_accu: 0.9539, dev_accu: 0.8305\n",
      "Epoch 22: train_loss 2.5449, train_accu: 0.9586, dev_accu: 0.8318\n",
      "Epoch 23: train_loss 2.3571, train_accu: 0.9637, dev_accu: 0.8336\n",
      "Epoch 24: train_loss 2.1858, train_accu: 0.9693, dev_accu: 0.8368\n"
     ]
    }
   ],
   "source": [
    "model_w1_feat = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                                  window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w1_feat, df_w1_feat = \\\n",
    "train_util(model_w1_feat, X_train_w1_feat, Y_train, X_dev_w1_feat, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8441\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1_feat(X_devtest_w1_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_vocab = []\n",
    "twitter_emb = []\n",
    "with open(DATADIR + 'twitter-embeddings.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split(' ')\n",
    "        word, emb = tokens[0], tokens[1:]\n",
    "        emb = [float(elm) for elm in emb]\n",
    "        twitter_vocab.append(word)\n",
    "        twitter_emb.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_emb = torch.tensor(twitter_emb)\n",
    "# for <s>, use the emb for </s>\n",
    "idx2word_pretrained = twitter_vocab + ['<s>']\n",
    "temp = twitter_emb[word2idx_pretrained['</s>']].view((1, -1))\n",
    "# construct maps for pretrained word embs\n",
    "twitter_emb = torch.cat((twitter_emb, temp))\n",
    "word2idx_pretrained = {word: idx for idx, word in enumerate(idx2word_pretrained)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, Devtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0_pre = encode_lines(train, word2idx_pretrained, window_size=0)\n",
    "X_dev_w0_pre = encode_lines(dev, word2idx_pretrained, window_size=0)\n",
    "X_devtest_w0_pre = encode_lines(devtest, word2idx_pretrained, window_size=0)\n",
    "\n",
    "# w = 1\n",
    "X_train_w1_pre = encode_lines(train, word2idx_pretrained, window_size=1)\n",
    "X_dev_w1_pre = encode_lines(dev, word2idx_pretrained, window_size=1)\n",
    "X_devtest_w1_pre = encode_lines(devtest, word2idx_pretrained, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 41.3262, train_accu: 0.6117, dev_accu: 0.6142\n",
      "Epoch 1: train_loss 22.9794, train_accu: 0.7210, dev_accu: 0.7119\n",
      "Epoch 2: train_loss 16.7514, train_accu: 0.7656, dev_accu: 0.7540\n",
      "Epoch 3: train_loss 13.9303, train_accu: 0.7954, dev_accu: 0.7822\n",
      "Epoch 4: train_loss 12.2596, train_accu: 0.8102, dev_accu: 0.7980\n",
      "Epoch 5: train_loss 11.1581, train_accu: 0.8147, dev_accu: 0.8007\n",
      "Epoch 6: train_loss 10.3877, train_accu: 0.8233, dev_accu: 0.8079\n",
      "Epoch 7: train_loss 9.8198, train_accu: 0.8292, dev_accu: 0.8133\n",
      "Epoch 8: train_loss 9.3837, train_accu: 0.8332, dev_accu: 0.8171\n",
      "Epoch 9: train_loss 9.0371, train_accu: 0.8371, dev_accu: 0.8185\n",
      "Epoch 10: train_loss 8.7520, train_accu: 0.8399, dev_accu: 0.8210\n",
      "Epoch 11: train_loss 8.5101, train_accu: 0.8423, dev_accu: 0.8231\n",
      "Epoch 12: train_loss 8.2993, train_accu: 0.8442, dev_accu: 0.8237\n",
      "Epoch 13: train_loss 8.1117, train_accu: 0.8469, dev_accu: 0.8241\n",
      "Epoch 14: train_loss 7.9424, train_accu: 0.8505, dev_accu: 0.8274\n",
      "Epoch 15: train_loss 7.7877, train_accu: 0.8534, dev_accu: 0.8303\n",
      "Epoch 16: train_loss 7.6452, train_accu: 0.8548, dev_accu: 0.8307\n",
      "Epoch 17: train_loss 7.5130, train_accu: 0.8571, dev_accu: 0.8305\n",
      "Epoch 18: train_loss 7.3896, train_accu: 0.8591, dev_accu: 0.8307\n",
      "Epoch 19: train_loss 7.2741, train_accu: 0.8609, dev_accu: 0.8303\n",
      "Epoch 20: train_loss 7.1656, train_accu: 0.8628, dev_accu: 0.8307\n",
      "Epoch 21: train_loss 7.0634, train_accu: 0.8642, dev_accu: 0.8305\n",
      "Epoch 22: train_loss 6.9670, train_accu: 0.8659, dev_accu: 0.8307\n",
      "Epoch 23: train_loss 6.8761, train_accu: 0.8669, dev_accu: 0.8314\n",
      "Epoch 24: train_loss 6.7904, train_accu: 0.8680, dev_accu: 0.8314\n"
     ]
    }
   ],
   "source": [
    "model_w0_tune = FeedForwardTagger(window_size=0, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                  pretrained_emb=twitter_emb)\n",
    "best_model_w0_tune, df_w0_tune = \\\n",
    "train_util(model_w0_tune, X_train_w0_pre, Y_train, X_dev_w0_pre, Y_dev, \n",
    "                                  n_epochs=25, lr=1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8353\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w0_tune(X_devtest_w0_pre), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0_tune = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 42.9440, train_accu: 0.4770, dev_accu: 0.4814\n",
      "Epoch 1: train_loss 31.8846, train_accu: 0.4953, dev_accu: 0.4991\n",
      "Epoch 2: train_loss 29.9445, train_accu: 0.5047, dev_accu: 0.5020\n",
      "Epoch 3: train_loss 28.8760, train_accu: 0.4903, dev_accu: 0.4883\n",
      "Epoch 4: train_loss 28.3043, train_accu: 0.4912, dev_accu: 0.4864\n",
      "Epoch 5: train_loss 27.8555, train_accu: 0.5149, dev_accu: 0.5067\n",
      "Epoch 6: train_loss 27.6793, train_accu: 0.5166, dev_accu: 0.5082\n",
      "Epoch 7: train_loss 27.4645, train_accu: 0.5205, dev_accu: 0.5119\n",
      "Epoch 8: train_loss 27.2929, train_accu: 0.5211, dev_accu: 0.5128\n",
      "Epoch 9: train_loss 27.1680, train_accu: 0.5192, dev_accu: 0.5125\n",
      "Epoch 10: train_loss 27.0628, train_accu: 0.5200, dev_accu: 0.5128\n",
      "Epoch 11: train_loss 26.9703, train_accu: 0.5203, dev_accu: 0.5125\n",
      "Epoch 12: train_loss 26.8874, train_accu: 0.5222, dev_accu: 0.5150\n",
      "Epoch 13: train_loss 26.8126, train_accu: 0.5227, dev_accu: 0.5148\n",
      "Epoch 14: train_loss 26.7444, train_accu: 0.5231, dev_accu: 0.5152\n",
      "Epoch 15: train_loss 26.6815, train_accu: 0.5246, dev_accu: 0.5163\n",
      "Epoch 16: train_loss 26.6230, train_accu: 0.5243, dev_accu: 0.5152\n",
      "Epoch 17: train_loss 26.5682, train_accu: 0.5271, dev_accu: 0.5206\n",
      "Epoch 18: train_loss 26.5166, train_accu: 0.5277, dev_accu: 0.5213\n",
      "Epoch 19: train_loss 26.4677, train_accu: 0.5286, dev_accu: 0.5221\n",
      "Epoch 20: train_loss 26.4214, train_accu: 0.5288, dev_accu: 0.5221\n",
      "Epoch 21: train_loss 26.3776, train_accu: 0.5296, dev_accu: 0.5227\n",
      "Epoch 22: train_loss 26.3382, train_accu: 0.5304, dev_accu: 0.5235\n",
      "Epoch 23: train_loss 26.3179, train_accu: 0.5306, dev_accu: 0.5217\n",
      "Epoch 24: train_loss 26.3190, train_accu: 0.5281, dev_accu: 0.5206\n",
      "Epoch 25: train_loss 26.3075, train_accu: 0.5243, dev_accu: 0.5165\n",
      "Epoch 26: train_loss 26.2354, train_accu: 0.5327, dev_accu: 0.5244\n",
      "Epoch 27: train_loss 26.1600, train_accu: 0.5339, dev_accu: 0.5192\n",
      "Epoch 28: train_loss 26.2049, train_accu: 0.5337, dev_accu: 0.5221\n",
      "Epoch 29: train_loss 26.1077, train_accu: 0.5343, dev_accu: 0.5225\n",
      "Epoch 30: train_loss 26.0711, train_accu: 0.5347, dev_accu: 0.5225\n",
      "Epoch 31: train_loss 26.0673, train_accu: 0.5358, dev_accu: 0.5198\n",
      "Epoch 32: train_loss 26.0585, train_accu: 0.5360, dev_accu: 0.5219\n",
      "Epoch 33: train_loss 25.9903, train_accu: 0.5285, dev_accu: 0.5177\n",
      "Epoch 34: train_loss 26.0594, train_accu: 0.5302, dev_accu: 0.5175\n",
      "Epoch 35: train_loss 25.9016, train_accu: 0.5375, dev_accu: 0.5227\n",
      "Epoch 36: train_loss 26.0315, train_accu: 0.5377, dev_accu: 0.5240\n",
      "Epoch 37: train_loss 25.8754, train_accu: 0.5380, dev_accu: 0.5238\n",
      "Epoch 38: train_loss 25.8525, train_accu: 0.5390, dev_accu: 0.5208\n",
      "Epoch 39: train_loss 25.8762, train_accu: 0.5391, dev_accu: 0.5233\n",
      "Epoch 40: train_loss 25.8193, train_accu: 0.5391, dev_accu: 0.5233\n",
      "Epoch 41: train_loss 25.8427, train_accu: 0.5323, dev_accu: 0.5173\n",
      "Epoch 42: train_loss 25.7926, train_accu: 0.5403, dev_accu: 0.5250\n",
      "Epoch 43: train_loss 25.8153, train_accu: 0.5415, dev_accu: 0.5219\n",
      "Epoch 44: train_loss 25.7366, train_accu: 0.5411, dev_accu: 0.5248\n",
      "Epoch 45: train_loss 25.6743, train_accu: 0.5416, dev_accu: 0.5250\n",
      "Epoch 46: train_loss 25.6562, train_accu: 0.5426, dev_accu: 0.5235\n",
      "Epoch 47: train_loss 25.7435, train_accu: 0.5430, dev_accu: 0.5267\n",
      "Epoch 48: train_loss 25.6294, train_accu: 0.5395, dev_accu: 0.5227\n",
      "Epoch 49: train_loss 25.6958, train_accu: 0.5356, dev_accu: 0.5198\n",
      "Epoch 50: train_loss 25.5889, train_accu: 0.5404, dev_accu: 0.5233\n",
      "Epoch 51: train_loss 25.5440, train_accu: 0.5173, dev_accu: 0.5001\n",
      "Epoch 52: train_loss 25.7734, train_accu: 0.5433, dev_accu: 0.5227\n",
      "Epoch 53: train_loss 25.5252, train_accu: 0.5445, dev_accu: 0.5248\n",
      "Epoch 54: train_loss 25.4887, train_accu: 0.5430, dev_accu: 0.5252\n",
      "Epoch 55: train_loss 25.5507, train_accu: 0.5402, dev_accu: 0.5225\n",
      "Epoch 56: train_loss 25.4866, train_accu: 0.5398, dev_accu: 0.5229\n",
      "Epoch 57: train_loss 25.4485, train_accu: 0.5408, dev_accu: 0.5231\n",
      "Epoch 58: train_loss 25.4072, train_accu: 0.5468, dev_accu: 0.5275\n",
      "Epoch 59: train_loss 25.5012, train_accu: 0.5140, dev_accu: 0.4928\n",
      "Epoch 60: train_loss 25.4778, train_accu: 0.5460, dev_accu: 0.5256\n",
      "Epoch 61: train_loss 25.3813, train_accu: 0.5456, dev_accu: 0.5250\n",
      "Epoch 62: train_loss 25.3766, train_accu: 0.5464, dev_accu: 0.5262\n",
      "Epoch 63: train_loss 25.4751, train_accu: 0.5204, dev_accu: 0.4966\n",
      "Epoch 64: train_loss 25.3522, train_accu: 0.5469, dev_accu: 0.5252\n",
      "Epoch 65: train_loss 25.3146, train_accu: 0.5472, dev_accu: 0.5252\n",
      "Epoch 66: train_loss 25.2619, train_accu: 0.5479, dev_accu: 0.5227\n",
      "Epoch 67: train_loss 25.4205, train_accu: 0.5214, dev_accu: 0.4964\n",
      "Epoch 68: train_loss 25.2833, train_accu: 0.5459, dev_accu: 0.5242\n",
      "Epoch 69: train_loss 25.2963, train_accu: 0.5451, dev_accu: 0.5229\n",
      "Epoch 70: train_loss 25.2586, train_accu: 0.5424, dev_accu: 0.5221\n",
      "Epoch 71: train_loss 25.1968, train_accu: 0.5500, dev_accu: 0.5287\n",
      "Epoch 72: train_loss 25.2040, train_accu: 0.5225, dev_accu: 0.4947\n",
      "Epoch 73: train_loss 25.2448, train_accu: 0.5490, dev_accu: 0.5229\n",
      "Epoch 74: train_loss 25.2318, train_accu: 0.5493, dev_accu: 0.5225\n",
      "Epoch 75: train_loss 25.1418, train_accu: 0.5215, dev_accu: 0.4922\n",
      "Epoch 76: train_loss 25.2169, train_accu: 0.5493, dev_accu: 0.5231\n",
      "Epoch 77: train_loss 25.0790, train_accu: 0.5495, dev_accu: 0.5194\n",
      "Epoch 78: train_loss 25.0648, train_accu: 0.5518, dev_accu: 0.5262\n",
      "Epoch 79: train_loss 24.9881, train_accu: 0.5486, dev_accu: 0.5221\n",
      "Epoch 80: train_loss 25.2499, train_accu: 0.5203, dev_accu: 0.4931\n",
      "Epoch 81: train_loss 25.2029, train_accu: 0.5501, dev_accu: 0.5188\n",
      "Epoch 82: train_loss 25.1066, train_accu: 0.5522, dev_accu: 0.5202\n",
      "Epoch 83: train_loss 24.9944, train_accu: 0.5501, dev_accu: 0.5215\n",
      "Epoch 84: train_loss 25.0096, train_accu: 0.5458, dev_accu: 0.5188\n",
      "Epoch 85: train_loss 25.0782, train_accu: 0.5482, dev_accu: 0.5196\n",
      "Epoch 86: train_loss 24.9216, train_accu: 0.5538, dev_accu: 0.5221\n",
      "Epoch 87: train_loss 24.9983, train_accu: 0.5558, dev_accu: 0.5283\n",
      "Epoch 88: train_loss 25.0327, train_accu: 0.5488, dev_accu: 0.5221\n",
      "Epoch 89: train_loss 25.0134, train_accu: 0.5537, dev_accu: 0.5229\n",
      "Epoch 90: train_loss 24.9189, train_accu: 0.5288, dev_accu: 0.4957\n",
      "Epoch 91: train_loss 24.9700, train_accu: 0.5543, dev_accu: 0.5181\n",
      "Epoch 92: train_loss 24.8403, train_accu: 0.5535, dev_accu: 0.5184\n",
      "Epoch 93: train_loss 24.8262, train_accu: 0.5551, dev_accu: 0.5250\n",
      "Epoch 94: train_loss 24.9241, train_accu: 0.5555, dev_accu: 0.5227\n",
      "Epoch 95: train_loss 24.8739, train_accu: 0.5469, dev_accu: 0.5190\n",
      "Epoch 96: train_loss 24.9334, train_accu: 0.5555, dev_accu: 0.5264\n",
      "Epoch 97: train_loss 24.7447, train_accu: 0.5551, dev_accu: 0.5221\n",
      "Epoch 98: train_loss 24.7184, train_accu: 0.5572, dev_accu: 0.5256\n",
      "Epoch 99: train_loss 24.8965, train_accu: 0.5595, dev_accu: 0.5267\n"
     ]
    }
   ],
   "source": [
    "model_w1_tune = FeedForwardTagger(window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                  pretrained_emb=twitter_emb)\n",
    "best_model_w1_tune, df_w1_tune = \\\n",
    "train_util(model_w1_tune, X_train_w1_pre, Y_train, X_dev_w1_pre, Y_dev, \n",
    "                                  n_epochs=100, lr=1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 41.6860, train_accu: 0.4962, dev_accu: 0.4987\n",
      "Epoch 1: train_loss 30.3371, train_accu: 0.5119, dev_accu: 0.5078\n",
      "Epoch 2: train_loss 28.5443, train_accu: 0.5241, dev_accu: 0.5125\n",
      "Epoch 3: train_loss 27.7047, train_accu: 0.5285, dev_accu: 0.5171\n",
      "Epoch 4: train_loss 27.2342, train_accu: 0.5321, dev_accu: 0.5169\n",
      "Epoch 5: train_loss 26.9277, train_accu: 0.5339, dev_accu: 0.5206\n",
      "Epoch 6: train_loss 26.7054, train_accu: 0.5349, dev_accu: 0.5217\n",
      "Epoch 7: train_loss 26.5317, train_accu: 0.5365, dev_accu: 0.5217\n",
      "Epoch 8: train_loss 26.3886, train_accu: 0.5374, dev_accu: 0.5231\n",
      "Epoch 9: train_loss 26.2663, train_accu: 0.5384, dev_accu: 0.5238\n",
      "Epoch 10: train_loss 26.1590, train_accu: 0.5405, dev_accu: 0.5240\n",
      "Epoch 11: train_loss 26.0634, train_accu: 0.5410, dev_accu: 0.5240\n",
      "Epoch 12: train_loss 25.9772, train_accu: 0.5414, dev_accu: 0.5248\n",
      "Epoch 13: train_loss 25.8986, train_accu: 0.5416, dev_accu: 0.5252\n",
      "Epoch 14: train_loss 25.8265, train_accu: 0.5424, dev_accu: 0.5258\n",
      "Epoch 15: train_loss 25.7600, train_accu: 0.5435, dev_accu: 0.5260\n",
      "Epoch 16: train_loss 25.6983, train_accu: 0.5441, dev_accu: 0.5260\n",
      "Epoch 17: train_loss 25.6410, train_accu: 0.5450, dev_accu: 0.5262\n",
      "Epoch 18: train_loss 25.5874, train_accu: 0.5451, dev_accu: 0.5260\n",
      "Epoch 19: train_loss 25.5373, train_accu: 0.5455, dev_accu: 0.5264\n",
      "Epoch 20: train_loss 25.4901, train_accu: 0.5462, dev_accu: 0.5260\n",
      "Epoch 21: train_loss 25.4456, train_accu: 0.5469, dev_accu: 0.5264\n",
      "Epoch 22: train_loss 25.4035, train_accu: 0.5474, dev_accu: 0.5264\n",
      "Epoch 23: train_loss 25.3637, train_accu: 0.5480, dev_accu: 0.5275\n",
      "Epoch 24: train_loss 25.3258, train_accu: 0.5483, dev_accu: 0.5277\n",
      "Epoch 25: train_loss 25.2897, train_accu: 0.5487, dev_accu: 0.5281\n",
      "Epoch 26: train_loss 25.2554, train_accu: 0.5497, dev_accu: 0.5283\n",
      "Epoch 27: train_loss 25.2226, train_accu: 0.5501, dev_accu: 0.5289\n",
      "Epoch 28: train_loss 25.1912, train_accu: 0.5504, dev_accu: 0.5296\n",
      "Epoch 29: train_loss 25.1612, train_accu: 0.5508, dev_accu: 0.5291\n",
      "Epoch 30: train_loss 25.1324, train_accu: 0.5515, dev_accu: 0.5291\n",
      "Epoch 31: train_loss 25.1047, train_accu: 0.5516, dev_accu: 0.5271\n",
      "Epoch 32: train_loss 25.0781, train_accu: 0.5520, dev_accu: 0.5273\n",
      "Epoch 33: train_loss 25.0526, train_accu: 0.5525, dev_accu: 0.5273\n",
      "Epoch 34: train_loss 25.0279, train_accu: 0.5528, dev_accu: 0.5275\n",
      "Epoch 35: train_loss 25.0041, train_accu: 0.5531, dev_accu: 0.5275\n",
      "Epoch 36: train_loss 24.9811, train_accu: 0.5534, dev_accu: 0.5279\n",
      "Epoch 37: train_loss 24.9588, train_accu: 0.5535, dev_accu: 0.5275\n",
      "Epoch 38: train_loss 24.9373, train_accu: 0.5539, dev_accu: 0.5271\n",
      "Epoch 39: train_loss 24.9164, train_accu: 0.5542, dev_accu: 0.5271\n",
      "Epoch 40: train_loss 24.8961, train_accu: 0.5543, dev_accu: 0.5277\n",
      "Epoch 41: train_loss 24.8763, train_accu: 0.5546, dev_accu: 0.5281\n",
      "Epoch 42: train_loss 24.8572, train_accu: 0.5549, dev_accu: 0.5281\n",
      "Epoch 43: train_loss 24.8385, train_accu: 0.5549, dev_accu: 0.5281\n",
      "Epoch 44: train_loss 24.8203, train_accu: 0.5550, dev_accu: 0.5279\n",
      "Epoch 45: train_loss 24.8025, train_accu: 0.5549, dev_accu: 0.5283\n",
      "Epoch 46: train_loss 24.7852, train_accu: 0.5553, dev_accu: 0.5279\n",
      "Epoch 47: train_loss 24.7682, train_accu: 0.5555, dev_accu: 0.5279\n",
      "Epoch 48: train_loss 24.7517, train_accu: 0.5553, dev_accu: 0.5308\n",
      "Epoch 49: train_loss 24.7355, train_accu: 0.5555, dev_accu: 0.5312\n",
      "Epoch 50: train_loss 24.7197, train_accu: 0.5558, dev_accu: 0.5312\n",
      "Epoch 51: train_loss 24.7042, train_accu: 0.5563, dev_accu: 0.5312\n",
      "Epoch 52: train_loss 24.6891, train_accu: 0.5567, dev_accu: 0.5314\n",
      "Epoch 53: train_loss 24.6742, train_accu: 0.5567, dev_accu: 0.5316\n",
      "Epoch 54: train_loss 24.6597, train_accu: 0.5569, dev_accu: 0.5316\n",
      "Epoch 55: train_loss 24.6454, train_accu: 0.5569, dev_accu: 0.5316\n",
      "Epoch 56: train_loss 24.6314, train_accu: 0.5569, dev_accu: 0.5316\n",
      "Epoch 57: train_loss 24.6177, train_accu: 0.5569, dev_accu: 0.5310\n",
      "Epoch 58: train_loss 24.6043, train_accu: 0.5572, dev_accu: 0.5314\n",
      "Epoch 59: train_loss 24.5911, train_accu: 0.5573, dev_accu: 0.5314\n",
      "Epoch 60: train_loss 24.5782, train_accu: 0.5574, dev_accu: 0.5314\n",
      "Epoch 61: train_loss 24.5654, train_accu: 0.5577, dev_accu: 0.5314\n",
      "Epoch 62: train_loss 24.5530, train_accu: 0.5580, dev_accu: 0.5314\n",
      "Epoch 63: train_loss 24.5407, train_accu: 0.5583, dev_accu: 0.5314\n",
      "Epoch 64: train_loss 24.5286, train_accu: 0.5583, dev_accu: 0.5314\n",
      "Epoch 65: train_loss 24.5168, train_accu: 0.5584, dev_accu: 0.5314\n",
      "Epoch 66: train_loss 24.5052, train_accu: 0.5584, dev_accu: 0.5312\n",
      "Epoch 67: train_loss 24.4937, train_accu: 0.5586, dev_accu: 0.5310\n",
      "Epoch 68: train_loss 24.4825, train_accu: 0.5588, dev_accu: 0.5310\n",
      "Epoch 69: train_loss 24.4714, train_accu: 0.5591, dev_accu: 0.5304\n",
      "Epoch 70: train_loss 24.4605, train_accu: 0.5593, dev_accu: 0.5304\n",
      "Epoch 71: train_loss 24.4498, train_accu: 0.5593, dev_accu: 0.5304\n",
      "Epoch 72: train_loss 24.4392, train_accu: 0.5594, dev_accu: 0.5304\n",
      "Epoch 73: train_loss 24.4288, train_accu: 0.5595, dev_accu: 0.5300\n",
      "Epoch 74: train_loss 24.4185, train_accu: 0.5598, dev_accu: 0.5300\n",
      "Epoch 75: train_loss 24.4084, train_accu: 0.5600, dev_accu: 0.5302\n",
      "Epoch 76: train_loss 24.3985, train_accu: 0.5600, dev_accu: 0.5302\n",
      "Epoch 77: train_loss 24.3887, train_accu: 0.5602, dev_accu: 0.5304\n",
      "Epoch 78: train_loss 24.3790, train_accu: 0.5604, dev_accu: 0.5302\n",
      "Epoch 79: train_loss 24.3695, train_accu: 0.5605, dev_accu: 0.5302\n",
      "Epoch 80: train_loss 24.3601, train_accu: 0.5607, dev_accu: 0.5306\n",
      "Epoch 81: train_loss 24.3508, train_accu: 0.5607, dev_accu: 0.5306\n",
      "Epoch 82: train_loss 24.3417, train_accu: 0.5608, dev_accu: 0.5304\n",
      "Epoch 83: train_loss 24.3326, train_accu: 0.5592, dev_accu: 0.5279\n",
      "Epoch 84: train_loss 24.3237, train_accu: 0.5594, dev_accu: 0.5285\n",
      "Epoch 85: train_loss 24.3149, train_accu: 0.5596, dev_accu: 0.5285\n",
      "Epoch 86: train_loss 24.3062, train_accu: 0.5601, dev_accu: 0.5277\n",
      "Epoch 87: train_loss 24.2977, train_accu: 0.5602, dev_accu: 0.5277\n",
      "Epoch 88: train_loss 24.2892, train_accu: 0.5602, dev_accu: 0.5279\n",
      "Epoch 89: train_loss 24.2808, train_accu: 0.5605, dev_accu: 0.5277\n",
      "Epoch 90: train_loss 24.2726, train_accu: 0.5606, dev_accu: 0.5275\n",
      "Epoch 91: train_loss 24.2644, train_accu: 0.5606, dev_accu: 0.5275\n",
      "Epoch 92: train_loss 24.2563, train_accu: 0.5604, dev_accu: 0.5273\n",
      "Epoch 93: train_loss 24.2483, train_accu: 0.5607, dev_accu: 0.5273\n",
      "Epoch 94: train_loss 24.2404, train_accu: 0.5607, dev_accu: 0.5271\n",
      "Epoch 95: train_loss 24.2326, train_accu: 0.5608, dev_accu: 0.5271\n",
      "Epoch 96: train_loss 24.2249, train_accu: 0.5608, dev_accu: 0.5271\n",
      "Epoch 97: train_loss 24.2173, train_accu: 0.5609, dev_accu: 0.5267\n",
      "Epoch 98: train_loss 24.2098, train_accu: 0.5611, dev_accu: 0.5269\n",
      "Epoch 99: train_loss 24.2023, train_accu: 0.5600, dev_accu: 0.5231\n"
     ]
    }
   ],
   "source": [
    "model_w1_freeze = FeedForwardTagger(window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                  pretrained_emb=twitter_emb, freeze=True)\n",
    "best_model_w1_freeze, df_w1_freeze = \\\n",
    "train_util(model_w1_freeze, X_train_w1_pre, Y_train, X_dev_w1_pre, Y_dev, \n",
    "                                  n_epochs=100, lr=1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imodel_w1_tune = FeedForwardTagger(window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                  pretrained_emb=twitter_emb)\n",
    "best_model_w1_tune, df_w1_tune = \\\n",
    "train_util(model_w1_tune, X_train_w1_pre, Y_train, X_dev_w1_pre, Y_dev, \n",
    "                                  n_epochs=100, lr=1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Architecture Engineering\n",
    "## w = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 2\n",
    "X_train_w2 = encode_lines(train, word2idx_rand, window_size=2)\n",
    "X_dev_w2 = encode_lines(dev, word2idx_rand, window_size=2)\n",
    "X_devtest_w2 = encode_lines(devtest, word2idx_rand, window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
