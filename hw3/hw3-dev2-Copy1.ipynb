{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe331513930>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data/'\n",
    "EMB_DIM = 50 # embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        lines: [['hello', 'world'], ...]\n",
    "        labels: [[!], [N], ...]\n",
    "        vocab\n",
    "    \"\"\"\n",
    "    with open(file, 'rt') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n\\n')\n",
    "    ret_lines = []\n",
    "    labels = []\n",
    "    vocab = set()\n",
    "    for line in lines:\n",
    "        if not line: \n",
    "            continue\n",
    "        curr_line = []\n",
    "        for token_label_str in line.split('\\n'):\n",
    "            if not token_label_str: \n",
    "                continue\n",
    "            token, label = token_label_str.split('\\t')\n",
    "            vocab.add(token)\n",
    "            labels.append(label)\n",
    "            curr_line.append(token)\n",
    "        ret_lines.append(curr_line)\n",
    "    return ret_lines, labels, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lines(lines, word2idx_map, window_size):\n",
    "    \"\"\"\n",
    "    returns X: len(lines) x (2 * window_size + 1)\n",
    "    \"\"\"\n",
    "    def encode_line(line, word2idx_map, window_size):\n",
    "        num_repr = [] # numerical representation\n",
    "        for word in line:\n",
    "            num = word2idx_map.get(word, word2idx_map['UUUNKKK'])\n",
    "            num_repr.append(num)\n",
    "        # pad with start and end tokens\n",
    "        start = [word2idx_map['<s>']] * window_size\n",
    "        end = [word2idx_map['</s>']] * window_size\n",
    "        padded = start + num_repr + end\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(window_size, len(padded) - window_size):\n",
    "            windowed = padded[i - window_size : i + window_size + 1]\n",
    "            ret.append(windowed)\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    res = []\n",
    "    for line in lines:\n",
    "        res.extend(encode_line(line, word2idx_map, window_size))\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(lines, features):\n",
    "    \"\"\"\n",
    "    features: a set of characters to look for in each token\n",
    "    also append to the end:\n",
    "        (count features are normalized during training)\n",
    "        - a count feature for digits\n",
    "        - a count feature for the length of the entire token\n",
    "    returns X: len(lines) x num_features\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            word_features = []\n",
    "            char_set = set(word)\n",
    "            for feature in features:\n",
    "                val = 1 if feature in char_set else 0\n",
    "                word_features.append(val)\n",
    "\n",
    "            word_features.append(len(re.findall(r'\\d', word))) # digit feature\n",
    "            word_features.append(len(word)) # count feature\n",
    "        \n",
    "            res.append(word_features)\n",
    "\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, window_size, output_dim, emb_dim=EMB_DIM, \n",
    "                 vocab_size=None, pretrained_emb=None, freeze=False,\n",
    "                num_binary_features=0, num_count_features=0):\n",
    "        \"\"\"\n",
    "        vocab_size is None when using pretrained emb\n",
    "        count features will be batch-normalized during forward\n",
    "        \"\"\"\n",
    "        \n",
    "        super(FeedForwardTagger, self).__init__()\n",
    "        \n",
    "        self.num_bin_feat = num_binary_features\n",
    "        self.num_cnt_feat = num_count_features\n",
    "        \n",
    "        if pretrained_emb is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "            torch.nn.init.uniform_(self.emb.weight, -0.01, 0.01)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrained_emb, freeze=freeze)\n",
    "        \n",
    "        input_dim = (2 * window_size + 1) * emb_dim\n",
    "        input_dim += self.num_bin_feat + self.num_cnt_feat\n",
    "        \n",
    "        if self.num_cnt_feat != 0:\n",
    "            self.batchnorm = nn.BatchNorm1d(self.num_cnt_feat)\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        input: [emb | binary features | count features]\n",
    "        \"\"\"\n",
    "        num_feat = self.num_bin_feat + self.num_cnt_feat\n",
    "        if num_feat != 0:\n",
    "            to_embed = inputs[:, :-num_feat]\n",
    "            bin_feats = inputs[:, -num_feat : -self.num_cnt_feat]\n",
    "            cnt_feats = inputs[:, -self.num_cnt_feat:]\n",
    "            \n",
    "            # embed up to num_extra_features\n",
    "            embeds = self.emb(to_embed).view((inputs.shape[0], -1))\n",
    "            # normalize count features\n",
    "            cnt_feats = self.batchnorm(cnt_feats.float())\n",
    "            # concat emb w/ extra features\n",
    "            x = torch.cat((embeds, bin_feats, cnt_feats), dim=1)\n",
    "        else:\n",
    "            x = self.emb(inputs).view((inputs.shape[0], -1))\n",
    "            \n",
    "        out = torch.tanh(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util(model, X_train, Y_train, X_dev, Y_dev, n_epochs, lr, \n",
    "              batch_size):\n",
    "    \"\"\"\n",
    "    returns: best_model, losses, train_accu_list, dev_accu_list\n",
    "    \"\"\"\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_model = None\n",
    "    best_dev_accu = 0\n",
    "    losses = []\n",
    "    train_accu_list, dev_accu_list = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(X_train[i : i + batch_size])\n",
    "            loss = loss_func(log_probs, Y_train[i : i + batch_size])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_preds = torch.argmax(model(X_train), dim=1)\n",
    "        train_accu = accuracy_score(Y_train, train_preds)\n",
    "        # evaluate on dev\n",
    "        dev_preds = torch.argmax(model(X_dev), dim=1)\n",
    "        dev_accu = accuracy_score(Y_dev, dev_preds)\n",
    "        \n",
    "        # early stopping, save the model if it has improved on dev\n",
    "        if dev_accu > best_dev_accu:\n",
    "            best_dev_accu = dev_accu\n",
    "            best_model = deepcopy(model)\n",
    "        \n",
    "        print('Epoch {}: train_loss {:.4f}, train_accu: {:.4f}, dev_accu: {:.4f}'\\\n",
    "              .format(epoch, epoch_loss, train_accu, dev_accu))\n",
    "        losses.append(epoch_loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        dev_accu_list.append(dev_accu)\n",
    "        \n",
    "    loss_accu_df = pd.DataFrame({\n",
    "        'epoch': range(n_epochs), \n",
    "        'loss': losses,\n",
    "        'train_accu': train_accu_list,\n",
    "        'dev_accu': dev_accu_list})\n",
    "        \n",
    "    return best_model, loss_accu_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accu(loss_accu_df_list, window_list):\n",
    "    \"\"\"\n",
    "    input: two lists of the same length, loss_accu_df, window\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for df, w in zip(loss_accu_df_list, window_list):\n",
    "        df1 = df.melt(\n",
    "            'epoch', value_vars=['loss']).assign(window=w, plot='loss')\n",
    "        df2 = df.melt(\n",
    "            'epoch', value_vars=['train_accu', 'dev_accu']).assign(window=w, plot='accu')\n",
    "        dfs.extend([df1, df2])\n",
    "    plot_df = pd.concat(dfs)\n",
    "\n",
    "    g = sns.FacetGrid(data=plot_df, row='plot', col='window', \n",
    "                      hue='variable', sharey=False)\n",
    "    g.map_dataframe(sns.lineplot, x='epoch', y='value')\n",
    "    g.add_legend()\n",
    "\n",
    "def plot_confusion_matrix(matrix, labels, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, \n",
    "                     annot=True, fmt='d', cmap='Blues')\n",
    "    ax.set_xlabel('Predictions')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, train_vocab = read_corpus(DATADIR + 'twpos-train.tsv')\n",
    "dev, dev_labels, dev_vocab = read_corpus(DATADIR + 'twpos-dev.tsv')\n",
    "devtest, devtest_labels, devtest_vocab = read_corpus(DATADIR + 'twpos-devtest.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.unique(train_labels)\n",
    "all_labels_devtest = np.unique(devtest_labels) # devtest labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "Y_train = label_encoder.transform(train_labels)\n",
    "Y_dev = label_encoder.transform(dev_labels)\n",
    "Y_devtest = label_encoder.transform(devtest_labels)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_dev = torch.tensor(Y_dev, dtype=torch.long)\n",
    "Y_devtest = torch.tensor(Y_devtest, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devtest labels\n",
    "all_labels_devtest = np.unique(devtest_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_vocab.copy()\n",
    "vocab.update(dev_vocab)\n",
    "vocab.update(devtest_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline w/ Randomly Initialized Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for randomly initialized embs\n",
    "idx2word_rand = sorted(vocab)\n",
    "idx2word_rand += ['<s>', '</s>', 'UUUNKKK']\n",
    "word2idx_rand = {word: idx for idx, word in enumerate(idx2word_rand)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, DevTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0 = encode_lines(train, word2idx_rand, window_size=0)\n",
    "X_dev_w0 = encode_lines(dev, word2idx_rand, window_size=0)\n",
    "X_devtest_w0 = encode_lines(devtest, word2idx_rand, window_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 1\n",
    "X_train_w1 = encode_lines(train, word2idx_rand, window_size=1)\n",
    "X_dev_w1 = encode_lines(dev, word2idx_rand, window_size=1)\n",
    "X_devtest_w1 = encode_lines(devtest, word2idx_rand, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 25.5599, train_accu: 0.1514, dev_accu: 0.1558\n",
      "Epoch 1: train_loss 23.7730, train_accu: 0.2286, dev_accu: 0.2340\n",
      "Epoch 2: train_loss 22.1405, train_accu: 0.2680, dev_accu: 0.2690\n",
      "Epoch 3: train_loss 20.1998, train_accu: 0.3710, dev_accu: 0.3727\n",
      "Epoch 4: train_loss 17.8418, train_accu: 0.4738, dev_accu: 0.4723\n",
      "Epoch 5: train_loss 15.4302, train_accu: 0.4986, dev_accu: 0.4960\n",
      "Epoch 6: train_loss 14.1085, train_accu: 0.5089, dev_accu: 0.5065\n",
      "Epoch 7: train_loss 13.2731, train_accu: 0.5258, dev_accu: 0.5248\n",
      "Epoch 8: train_loss 12.4983, train_accu: 0.5381, dev_accu: 0.5403\n",
      "Epoch 9: train_loss 11.7002, train_accu: 0.6463, dev_accu: 0.6304\n",
      "Epoch 10: train_loss 11.0439, train_accu: 0.5687, dev_accu: 0.5671\n",
      "Epoch 11: train_loss 10.4354, train_accu: 0.6447, dev_accu: 0.6320\n",
      "Epoch 12: train_loss 9.8166, train_accu: 0.7130, dev_accu: 0.6743\n",
      "Epoch 13: train_loss 8.9609, train_accu: 0.6921, dev_accu: 0.6737\n",
      "Epoch 14: train_loss 8.3989, train_accu: 0.7376, dev_accu: 0.6988\n",
      "Epoch 15: train_loss 7.7715, train_accu: 0.7280, dev_accu: 0.6984\n",
      "Epoch 16: train_loss 7.2619, train_accu: 0.7608, dev_accu: 0.7127\n",
      "Epoch 17: train_loss 6.7517, train_accu: 0.7679, dev_accu: 0.7187\n",
      "Epoch 18: train_loss 6.2951, train_accu: 0.7825, dev_accu: 0.7293\n",
      "Epoch 19: train_loss 5.8800, train_accu: 0.7897, dev_accu: 0.7359\n",
      "Epoch 20: train_loss 5.5177, train_accu: 0.8097, dev_accu: 0.7449\n",
      "Epoch 21: train_loss 5.2040, train_accu: 0.8238, dev_accu: 0.7480\n",
      "Epoch 22: train_loss 4.9066, train_accu: 0.8543, dev_accu: 0.7540\n",
      "Epoch 23: train_loss 4.6479, train_accu: 0.8622, dev_accu: 0.7548\n",
      "Epoch 24: train_loss 4.3913, train_accu: 0.8684, dev_accu: 0.7521\n"
     ]
    }
   ],
   "source": [
    "model_w0 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=0,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w0, df_w0 = train_util(model_w0, X_train_w0, Y_train, X_dev_w0, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on devtest\n",
    "devtest_preds = torch.argmax(best_model_w0(X_devtest_w0), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w1 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=1,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w1, df_w1 = train_util(model_w1, X_train_w1, Y_train, X_dev_w1, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1(X_devtest_w1), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses and accuracy, and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accu([df_w0, df_w1], window_list=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_w0, all_labels_devtest, 'w=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_w0, all_labels_devtest, 'w=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n",
    "In addition to the following binary features, I also added a count feature for digits and a count feature for the length of the entire token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['#', '%', \"'\", '/', ':', '’']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = construct_features(train, FEATURES)\n",
    "X_dev_features = construct_features(dev, FEATURES)\n",
    "X_devtest_features = construct_features(devtest, FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w0_feat = torch.cat((X_train_w0, X_train_features), dim=1)\n",
    "X_dev_w0_feat = torch.cat((X_dev_w0, X_dev_features), dim=1)\n",
    "X_devtest_w0_feat = torch.cat((X_devtest_w0, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w1_feat = torch.cat((X_train_w1, X_train_features), dim=1)\n",
    "X_dev_w1_feat = torch.cat((X_dev_w1, X_dev_features), dim=1)\n",
    "X_devtest_w1_feat = torch.cat((X_devtest_w1, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 44.4099, train_accu: 0.2244, dev_accu: 0.2358\n",
      "Epoch 1: train_loss 39.6456, train_accu: 0.3888, dev_accu: 0.3804\n",
      "Epoch 2: train_loss 29.9395, train_accu: 0.5029, dev_accu: 0.5005\n",
      "Epoch 3: train_loss 24.2968, train_accu: 0.5964, dev_accu: 0.5878\n",
      "Epoch 4: train_loss 20.8838, train_accu: 0.6673, dev_accu: 0.6636\n",
      "Epoch 5: train_loss 18.6234, train_accu: 0.6963, dev_accu: 0.6816\n",
      "Epoch 6: train_loss 15.9197, train_accu: 0.7255, dev_accu: 0.7071\n",
      "Epoch 7: train_loss 13.9421, train_accu: 0.7529, dev_accu: 0.7308\n",
      "Epoch 8: train_loss 12.3033, train_accu: 0.8041, dev_accu: 0.7525\n",
      "Epoch 9: train_loss 10.8349, train_accu: 0.8329, dev_accu: 0.7635\n",
      "Epoch 10: train_loss 9.4517, train_accu: 0.8639, dev_accu: 0.7693\n",
      "Epoch 11: train_loss 8.4453, train_accu: 0.8741, dev_accu: 0.7714\n",
      "Epoch 12: train_loss 7.7078, train_accu: 0.8858, dev_accu: 0.7789\n",
      "Epoch 13: train_loss 6.8678, train_accu: 0.8987, dev_accu: 0.7864\n",
      "Epoch 14: train_loss 6.2967, train_accu: 0.9034, dev_accu: 0.7861\n",
      "Epoch 15: train_loss 5.8778, train_accu: 0.9086, dev_accu: 0.7855\n",
      "Epoch 16: train_loss 5.5369, train_accu: 0.9114, dev_accu: 0.7882\n",
      "Epoch 17: train_loss 5.2782, train_accu: 0.9137, dev_accu: 0.7899\n",
      "Epoch 18: train_loss 5.0617, train_accu: 0.9144, dev_accu: 0.7907\n",
      "Epoch 19: train_loss 4.8953, train_accu: 0.9154, dev_accu: 0.7917\n",
      "Epoch 20: train_loss 4.7401, train_accu: 0.9168, dev_accu: 0.7920\n",
      "Epoch 21: train_loss 4.6185, train_accu: 0.9183, dev_accu: 0.7917\n",
      "Epoch 22: train_loss 4.5120, train_accu: 0.9187, dev_accu: 0.7938\n",
      "Epoch 23: train_loss 4.4225, train_accu: 0.9193, dev_accu: 0.7928\n",
      "Epoch 24: train_loss 4.3369, train_accu: 0.9196, dev_accu: 0.7936\n"
     ]
    }
   ],
   "source": [
    "model_w0_feat = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                                  window_size=0, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w0_feat, df_w0_feat = \\\n",
    "train_util(model_w0_feat, X_train_w0_feat, Y_train, X_dev_w0_feat, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8021\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w0_feat(X_devtest_w0_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 45.1374, train_accu: 0.1795, dev_accu: 0.1854\n",
      "Epoch 1: train_loss 39.4513, train_accu: 0.2642, dev_accu: 0.2539\n",
      "Epoch 2: train_loss 34.2350, train_accu: 0.3862, dev_accu: 0.3748\n",
      "Epoch 3: train_loss 25.4433, train_accu: 0.6241, dev_accu: 0.6169\n",
      "Epoch 4: train_loss 19.7850, train_accu: 0.6755, dev_accu: 0.6677\n",
      "Epoch 5: train_loss 16.8015, train_accu: 0.7195, dev_accu: 0.7019\n",
      "Epoch 6: train_loss 14.6386, train_accu: 0.7490, dev_accu: 0.7283\n",
      "Epoch 7: train_loss 12.7468, train_accu: 0.7805, dev_accu: 0.7484\n",
      "Epoch 8: train_loss 11.1426, train_accu: 0.8099, dev_accu: 0.7650\n",
      "Epoch 9: train_loss 9.8024, train_accu: 0.8365, dev_accu: 0.7772\n",
      "Epoch 10: train_loss 8.6743, train_accu: 0.8507, dev_accu: 0.7818\n",
      "Epoch 11: train_loss 7.5694, train_accu: 0.8794, dev_accu: 0.7963\n",
      "Epoch 12: train_loss 6.6847, train_accu: 0.8977, dev_accu: 0.8023\n",
      "Epoch 13: train_loss 5.9266, train_accu: 0.9105, dev_accu: 0.8108\n",
      "Epoch 14: train_loss 5.2365, train_accu: 0.9197, dev_accu: 0.8146\n",
      "Epoch 15: train_loss 4.7002, train_accu: 0.9298, dev_accu: 0.8214\n",
      "Epoch 16: train_loss 4.2559, train_accu: 0.9352, dev_accu: 0.8241\n",
      "Epoch 17: train_loss 3.8744, train_accu: 0.9408, dev_accu: 0.8283\n",
      "Epoch 18: train_loss 3.5379, train_accu: 0.9445, dev_accu: 0.8295\n",
      "Epoch 19: train_loss 3.2419, train_accu: 0.9479, dev_accu: 0.8307\n",
      "Epoch 20: train_loss 2.9827, train_accu: 0.9504, dev_accu: 0.8314\n",
      "Epoch 21: train_loss 2.7522, train_accu: 0.9539, dev_accu: 0.8305\n",
      "Epoch 22: train_loss 2.5449, train_accu: 0.9586, dev_accu: 0.8318\n",
      "Epoch 23: train_loss 2.3571, train_accu: 0.9637, dev_accu: 0.8336\n",
      "Epoch 24: train_loss 2.1858, train_accu: 0.9693, dev_accu: 0.8368\n"
     ]
    }
   ],
   "source": [
    "model_w1_feat = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                                  window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w1_feat, df_w1_feat = \\\n",
    "train_util(model_w1_feat, X_train_w1_feat, Y_train, X_dev_w1_feat, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8441\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1_feat(X_devtest_w1_feat), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1_feat = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_vocab = []\n",
    "twitter_emb = []\n",
    "with open(DATADIR + 'twitter-embeddings.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split(' ')\n",
    "        word, emb = tokens[0], tokens[1:]\n",
    "        emb = [float(elm) for elm in emb]\n",
    "        twitter_vocab.append(word)\n",
    "        twitter_emb.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_emb = torch.tensor(twitter_emb)\n",
    "# for <s>, use the emb for </s>\n",
    "idx2word_pretrained = twitter_vocab + ['<s>']\n",
    "temp = twitter_emb[word2idx_pretrained['</s>']].view((1, -1))\n",
    "# construct maps for pretrained word embs\n",
    "twitter_emb = torch.cat((twitter_emb, temp))\n",
    "word2idx_pretrained = {word: idx for idx, word in enumerate(idx2word_pretrained)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, Devtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0_pre = encode_lines(train, word2idx_pretrained, window_size=0)\n",
    "X_dev_w0_pre = encode_lines(dev, word2idx_pretrained, window_size=0)\n",
    "X_devtest_w0_pre = encode_lines(devtest, word2idx_pretrained, window_size=0)\n",
    "\n",
    "# w = 1\n",
    "X_train_w1_pre = encode_lines(train, word2idx_pretrained, window_size=1)\n",
    "X_dev_w1_pre = encode_lines(dev, word2idx_pretrained, window_size=1)\n",
    "X_devtest_w1_pre = encode_lines(devtest, word2idx_pretrained, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 41.3375, train_accu: 0.6489, dev_accu: 0.6380\n",
      "Epoch 1: train_loss 21.0796, train_accu: 0.7587, dev_accu: 0.7306\n",
      "Epoch 2: train_loss 14.6463, train_accu: 0.8270, dev_accu: 0.7864\n",
      "Epoch 3: train_loss 11.5945, train_accu: 0.8475, dev_accu: 0.7998\n",
      "Epoch 4: train_loss 9.8695, train_accu: 0.8653, dev_accu: 0.8158\n",
      "Epoch 5: train_loss 8.7981, train_accu: 0.8726, dev_accu: 0.8222\n",
      "Epoch 6: train_loss 8.0842, train_accu: 0.8772, dev_accu: 0.8260\n",
      "Epoch 7: train_loss 7.5828, train_accu: 0.8817, dev_accu: 0.8297\n",
      "Epoch 8: train_loss 7.2167, train_accu: 0.8850, dev_accu: 0.8339\n",
      "Epoch 9: train_loss 6.9412, train_accu: 0.8791, dev_accu: 0.8301\n",
      "Epoch 10: train_loss 6.7282, train_accu: 0.8814, dev_accu: 0.8322\n",
      "Epoch 11: train_loss 6.5592, train_accu: 0.8824, dev_accu: 0.8332\n",
      "Epoch 12: train_loss 6.4219, train_accu: 0.8834, dev_accu: 0.8341\n",
      "Epoch 13: train_loss 6.3080, train_accu: 0.8844, dev_accu: 0.8351\n",
      "Epoch 14: train_loss 6.2117, train_accu: 0.8853, dev_accu: 0.8351\n",
      "Epoch 15: train_loss 6.1292, train_accu: 0.8856, dev_accu: 0.8353\n",
      "Epoch 16: train_loss 6.0576, train_accu: 0.8862, dev_accu: 0.8357\n",
      "Epoch 17: train_loss 5.9947, train_accu: 0.8866, dev_accu: 0.8359\n",
      "Epoch 18: train_loss 5.9390, train_accu: 0.8870, dev_accu: 0.8355\n",
      "Epoch 19: train_loss 5.8892, train_accu: 0.8872, dev_accu: 0.8359\n",
      "Epoch 20: train_loss 5.8444, train_accu: 0.8873, dev_accu: 0.8361\n",
      "Epoch 21: train_loss 5.8039, train_accu: 0.8873, dev_accu: 0.8359\n",
      "Epoch 22: train_loss 5.7669, train_accu: 0.8877, dev_accu: 0.8361\n",
      "Epoch 23: train_loss 5.7330, train_accu: 0.8881, dev_accu: 0.8365\n",
      "Epoch 24: train_loss 5.7018, train_accu: 0.8800, dev_accu: 0.8253\n"
     ]
    }
   ],
   "source": [
    "model_w0_tune = FeedForwardTagger(window_size=0, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                  pretrained_emb=twitter_emb)\n",
    "best_model_w0_tune, df_w0_tune = \\\n",
    "train_util(model_w0_tune, X_train_w0_pre, Y_train, X_dev_w0_pre, Y_dev, \n",
    "                                  n_epochs=25, lr=0.5, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8362\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w0_tune(X_devtest_w0_pre), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w0_tune = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 43.0116, train_accu: 0.6402, dev_accu: 0.6246\n",
      "Epoch 1: train_loss 22.6771, train_accu: 0.7425, dev_accu: 0.7156\n",
      "Epoch 2: train_loss 15.2831, train_accu: 0.8259, dev_accu: 0.7868\n",
      "Epoch 3: train_loss 11.5952, train_accu: 0.8640, dev_accu: 0.8183\n",
      "Epoch 4: train_loss 9.4116, train_accu: 0.8842, dev_accu: 0.8299\n",
      "Epoch 5: train_loss 8.0289, train_accu: 0.8972, dev_accu: 0.8405\n",
      "Epoch 6: train_loss 7.0805, train_accu: 0.9078, dev_accu: 0.8492\n",
      "Epoch 7: train_loss 6.3909, train_accu: 0.9172, dev_accu: 0.8579\n",
      "Epoch 8: train_loss 5.8713, train_accu: 0.9218, dev_accu: 0.8627\n",
      "Epoch 9: train_loss 5.4698, train_accu: 0.9265, dev_accu: 0.8654\n",
      "Epoch 10: train_loss 5.1525, train_accu: 0.9302, dev_accu: 0.8672\n",
      "Epoch 11: train_loss 4.8964, train_accu: 0.9326, dev_accu: 0.8708\n",
      "Epoch 12: train_loss 4.6851, train_accu: 0.9351, dev_accu: 0.8722\n",
      "Epoch 13: train_loss 4.5071, train_accu: 0.9372, dev_accu: 0.8731\n",
      "Epoch 14: train_loss 4.3546, train_accu: 0.9386, dev_accu: 0.8747\n",
      "Epoch 15: train_loss 4.2217, train_accu: 0.9406, dev_accu: 0.8774\n",
      "Epoch 16: train_loss 4.1044, train_accu: 0.9414, dev_accu: 0.8776\n",
      "Epoch 17: train_loss 3.9997, train_accu: 0.9426, dev_accu: 0.8787\n",
      "Epoch 18: train_loss 3.9053, train_accu: 0.9435, dev_accu: 0.8789\n",
      "Epoch 19: train_loss 3.8195, train_accu: 0.9442, dev_accu: 0.8793\n",
      "Epoch 20: train_loss 3.7409, train_accu: 0.9450, dev_accu: 0.8793\n",
      "Epoch 21: train_loss 3.6684, train_accu: 0.9458, dev_accu: 0.8797\n",
      "Epoch 22: train_loss 3.6012, train_accu: 0.9463, dev_accu: 0.8797\n",
      "Epoch 23: train_loss 3.5385, train_accu: 0.9469, dev_accu: 0.8801\n",
      "Epoch 24: train_loss 3.4798, train_accu: 0.9474, dev_accu: 0.8797\n"
     ]
    }
   ],
   "source": [
    "model_w1_tune = FeedForwardTagger(window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                  pretrained_emb=twitter_emb)\n",
    "best_model_w1_tune, df_w1_tune = \\\n",
    "train_util(model_w1_tune, X_train_w1_pre, Y_train, X_dev_w1_pre, Y_dev, \n",
    "                                  n_epochs=25, lr=0.5, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8866\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1_tune(X_devtest_w1_pre), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1_tune = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 43.4228, train_accu: 0.6442, dev_accu: 0.6322\n",
      "Epoch 1: train_loss 22.9755, train_accu: 0.7451, dev_accu: 0.7185\n",
      "Epoch 2: train_loss 15.4768, train_accu: 0.8243, dev_accu: 0.7810\n",
      "Epoch 3: train_loss 11.8207, train_accu: 0.8623, dev_accu: 0.8162\n",
      "Epoch 4: train_loss 9.6331, train_accu: 0.8804, dev_accu: 0.8293\n",
      "Epoch 5: train_loss 8.2220, train_accu: 0.8945, dev_accu: 0.8392\n",
      "Epoch 6: train_loss 7.2455, train_accu: 0.9048, dev_accu: 0.8459\n",
      "Epoch 7: train_loss 6.5327, train_accu: 0.9132, dev_accu: 0.8533\n",
      "Epoch 8: train_loss 5.9938, train_accu: 0.9194, dev_accu: 0.8594\n",
      "Epoch 9: train_loss 5.5757, train_accu: 0.9248, dev_accu: 0.8635\n",
      "Epoch 10: train_loss 5.2443, train_accu: 0.9291, dev_accu: 0.8664\n",
      "Epoch 11: train_loss 4.9770, train_accu: 0.9319, dev_accu: 0.8699\n",
      "Epoch 12: train_loss 4.7574, train_accu: 0.9344, dev_accu: 0.8699\n",
      "Epoch 13: train_loss 4.5739, train_accu: 0.9361, dev_accu: 0.8714\n",
      "Epoch 14: train_loss 4.4177, train_accu: 0.9384, dev_accu: 0.8743\n",
      "Epoch 15: train_loss 4.2828, train_accu: 0.9393, dev_accu: 0.8749\n",
      "Epoch 16: train_loss 4.1647, train_accu: 0.9408, dev_accu: 0.8758\n",
      "Epoch 17: train_loss 4.0602, train_accu: 0.9416, dev_accu: 0.8764\n",
      "Epoch 18: train_loss 3.9668, train_accu: 0.9427, dev_accu: 0.8776\n",
      "Epoch 19: train_loss 3.8827, train_accu: 0.9441, dev_accu: 0.8778\n",
      "Epoch 20: train_loss 3.8063, train_accu: 0.9450, dev_accu: 0.8789\n",
      "Epoch 21: train_loss 3.7366, train_accu: 0.9457, dev_accu: 0.8793\n",
      "Epoch 22: train_loss 3.6727, train_accu: 0.9457, dev_accu: 0.8797\n",
      "Epoch 23: train_loss 3.6136, train_accu: 0.9459, dev_accu: 0.8791\n",
      "Epoch 24: train_loss 3.5589, train_accu: 0.9464, dev_accu: 0.8787\n"
     ]
    }
   ],
   "source": [
    "model_w1_freeze = FeedForwardTagger(window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                  pretrained_emb=twitter_emb, freeze=True)\n",
    "best_model_w1_freeze, df_w1_freeze = \\\n",
    "train_util(model_w1_freeze, X_train_w1_pre, Y_train, X_dev_w1_pre, Y_dev, \n",
    "                                  n_epochs=25, lr=0.5, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8832\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1_freeze(X_devtest_w1_pre), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1_freeze = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w1_feat_pre = torch.cat((X_train_w1_pre, X_train_features), dim=1)\n",
    "X_dev_w1_feat_pre = torch.cat((X_dev_w1_pre, X_dev_features), dim=1)\n",
    "X_devtest_w1_feat_pre = torch.cat((X_devtest_w1_pre, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 39.8018, train_accu: 0.6590, dev_accu: 0.6432\n",
      "Epoch 1: train_loss 19.2448, train_accu: 0.7908, dev_accu: 0.7642\n",
      "Epoch 2: train_loss 12.7845, train_accu: 0.8592, dev_accu: 0.8189\n",
      "Epoch 3: train_loss 9.6095, train_accu: 0.8886, dev_accu: 0.8386\n",
      "Epoch 4: train_loss 7.8084, train_accu: 0.9058, dev_accu: 0.8504\n",
      "Epoch 5: train_loss 6.6785, train_accu: 0.9156, dev_accu: 0.8540\n",
      "Epoch 6: train_loss 5.9093, train_accu: 0.9228, dev_accu: 0.8602\n",
      "Epoch 7: train_loss 5.3505, train_accu: 0.9286, dev_accu: 0.8668\n",
      "Epoch 8: train_loss 4.9259, train_accu: 0.9327, dev_accu: 0.8731\n",
      "Epoch 9: train_loss 4.5934, train_accu: 0.9375, dev_accu: 0.8751\n",
      "Epoch 10: train_loss 4.3263, train_accu: 0.9412, dev_accu: 0.8780\n",
      "Epoch 11: train_loss 4.1067, train_accu: 0.9440, dev_accu: 0.8787\n",
      "Epoch 12: train_loss 3.9223, train_accu: 0.9466, dev_accu: 0.8795\n",
      "Epoch 13: train_loss 3.7645, train_accu: 0.9486, dev_accu: 0.8814\n",
      "Epoch 14: train_loss 3.6274, train_accu: 0.9497, dev_accu: 0.8830\n",
      "Epoch 15: train_loss 3.5066, train_accu: 0.9504, dev_accu: 0.8849\n",
      "Epoch 16: train_loss 3.3990, train_accu: 0.9514, dev_accu: 0.8849\n",
      "Epoch 17: train_loss 3.3022, train_accu: 0.9525, dev_accu: 0.8851\n",
      "Epoch 18: train_loss 3.2144, train_accu: 0.9531, dev_accu: 0.8855\n",
      "Epoch 19: train_loss 3.1341, train_accu: 0.9539, dev_accu: 0.8865\n",
      "Epoch 20: train_loss 3.0603, train_accu: 0.9549, dev_accu: 0.8865\n",
      "Epoch 21: train_loss 2.9921, train_accu: 0.9559, dev_accu: 0.8855\n",
      "Epoch 22: train_loss 2.9286, train_accu: 0.9571, dev_accu: 0.8857\n",
      "Epoch 23: train_loss 2.8693, train_accu: 0.9579, dev_accu: 0.8849\n",
      "Epoch 24: train_loss 2.8137, train_accu: 0.9587, dev_accu: 0.8849\n"
     ]
    }
   ],
   "source": [
    "model_w1_feat_tune = FeedForwardTagger(window_size=1, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                       pretrained_emb=twitter_emb,\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w1_feat_tune, df_w1_feat_tune = \\\n",
    "train_util(model_w1_feat_tune, X_train_w1_feat_pre, Y_train, X_dev_w1_feat_pre, Y_dev, \n",
    "                                  n_epochs=25, lr=0.5, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8935\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w1_feat_tune(X_devtest_w1_feat_pre), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w1_feat_tune = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Architecture Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w=2, random emb, no features baseline\n",
    "Slightly better than w=0 or 1 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2 = encode_lines(train, word2idx_rand, window_size=2)\n",
    "X_dev_w2 = encode_lines(dev, word2idx_rand, window_size=2)\n",
    "X_devtest_w2 = encode_lines(devtest, word2idx_rand, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 50.0884, train_accu: 0.1172, dev_accu: 0.1037\n",
      "Epoch 1: train_loss 46.5405, train_accu: 0.2074, dev_accu: 0.1998\n",
      "Epoch 2: train_loss 37.5295, train_accu: 0.4801, dev_accu: 0.4754\n",
      "Epoch 3: train_loss 27.3074, train_accu: 0.6084, dev_accu: 0.5997\n",
      "Epoch 4: train_loss 22.2459, train_accu: 0.6507, dev_accu: 0.6401\n",
      "Epoch 5: train_loss 19.6511, train_accu: 0.6891, dev_accu: 0.6716\n",
      "Epoch 6: train_loss 17.2149, train_accu: 0.7301, dev_accu: 0.7057\n",
      "Epoch 7: train_loss 14.8958, train_accu: 0.7721, dev_accu: 0.7366\n",
      "Epoch 8: train_loss 12.9335, train_accu: 0.8093, dev_accu: 0.7571\n",
      "Epoch 9: train_loss 11.2575, train_accu: 0.8377, dev_accu: 0.7700\n",
      "Epoch 10: train_loss 9.8069, train_accu: 0.8634, dev_accu: 0.7814\n",
      "Epoch 11: train_loss 8.5472, train_accu: 0.8847, dev_accu: 0.7880\n",
      "Epoch 12: train_loss 7.4676, train_accu: 0.9030, dev_accu: 0.7961\n",
      "Epoch 13: train_loss 6.5649, train_accu: 0.9165, dev_accu: 0.8042\n",
      "Epoch 14: train_loss 5.8176, train_accu: 0.9262, dev_accu: 0.8110\n",
      "Epoch 15: train_loss 5.1919, train_accu: 0.9343, dev_accu: 0.8148\n",
      "Epoch 16: train_loss 4.6589, train_accu: 0.9438, dev_accu: 0.8173\n",
      "Epoch 17: train_loss 4.2023, train_accu: 0.9513, dev_accu: 0.8166\n",
      "Epoch 18: train_loss 3.8132, train_accu: 0.9572, dev_accu: 0.8206\n",
      "Epoch 19: train_loss 3.4688, train_accu: 0.9614, dev_accu: 0.8210\n",
      "Epoch 20: train_loss 3.1580, train_accu: 0.9648, dev_accu: 0.8222\n",
      "Epoch 21: train_loss 2.8805, train_accu: 0.9682, dev_accu: 0.8224\n",
      "Epoch 22: train_loss 2.6361, train_accu: 0.9705, dev_accu: 0.8233\n",
      "Epoch 23: train_loss 2.4177, train_accu: 0.9734, dev_accu: 0.8233\n",
      "Epoch 24: train_loss 2.2212, train_accu: 0.9756, dev_accu: 0.8224\n"
     ]
    }
   ],
   "source": [
    "model_w2 = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=2,\n",
    "                          output_dim=len(all_labels))\n",
    "best_model_w2, df_w2 = train_util(model_w2, X_train_w2, Y_train, X_dev_w2, Y_dev, \n",
    "                                  n_epochs=25, lr=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8267\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w2(X_devtest_w2), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w2 = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w=2, using best config so far: pretrained emb, fine-tuning, features\n",
    "\n",
    "Not too different from w=1, pretrained, fine-tuning, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2_pre = encode_lines(train, word2idx_pretrained, window_size=2)\n",
    "X_dev_w2_pre = encode_lines(dev, word2idx_pretrained, window_size=2)\n",
    "X_devtest_w2_pre = encode_lines(devtest, word2idx_pretrained, window_size=2)\n",
    "X_train_w2_feat_pre = torch.cat((X_train_w2_pre, X_train_features), dim=1)\n",
    "X_dev_w2_feat_pre = torch.cat((X_dev_w2_pre, X_dev_features), dim=1)\n",
    "X_devtest_w2_feat_pre = torch.cat((X_devtest_w2_pre, X_devtest_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss 40.5641, train_accu: 0.6668, dev_accu: 0.6495\n",
      "Epoch 1: train_loss 19.5058, train_accu: 0.7853, dev_accu: 0.7540\n",
      "Epoch 2: train_loss 12.8262, train_accu: 0.8566, dev_accu: 0.8121\n",
      "Epoch 3: train_loss 9.5552, train_accu: 0.8905, dev_accu: 0.8372\n",
      "Epoch 4: train_loss 7.6819, train_accu: 0.9074, dev_accu: 0.8529\n",
      "Epoch 5: train_loss 6.5106, train_accu: 0.9180, dev_accu: 0.8598\n",
      "Epoch 6: train_loss 5.7169, train_accu: 0.9255, dev_accu: 0.8641\n",
      "Epoch 7: train_loss 5.1424, train_accu: 0.9323, dev_accu: 0.8720\n",
      "Epoch 8: train_loss 4.7072, train_accu: 0.9372, dev_accu: 0.8751\n",
      "Epoch 9: train_loss 4.3673, train_accu: 0.9413, dev_accu: 0.8766\n",
      "Epoch 10: train_loss 4.0948, train_accu: 0.9441, dev_accu: 0.8778\n",
      "Epoch 11: train_loss 3.8709, train_accu: 0.9474, dev_accu: 0.8791\n",
      "Epoch 12: train_loss 3.6828, train_accu: 0.9500, dev_accu: 0.8805\n",
      "Epoch 13: train_loss 3.5217, train_accu: 0.9511, dev_accu: 0.8816\n",
      "Epoch 14: train_loss 3.3814, train_accu: 0.9527, dev_accu: 0.8820\n",
      "Epoch 15: train_loss 3.2576, train_accu: 0.9543, dev_accu: 0.8830\n",
      "Epoch 16: train_loss 3.1470, train_accu: 0.9560, dev_accu: 0.8836\n",
      "Epoch 17: train_loss 3.0474, train_accu: 0.9566, dev_accu: 0.8836\n",
      "Epoch 18: train_loss 2.9567, train_accu: 0.9578, dev_accu: 0.8843\n",
      "Epoch 19: train_loss 2.8736, train_accu: 0.9587, dev_accu: 0.8845\n",
      "Epoch 20: train_loss 2.7969, train_accu: 0.9593, dev_accu: 0.8838\n",
      "Epoch 21: train_loss 2.7257, train_accu: 0.9604, dev_accu: 0.8830\n",
      "Epoch 22: train_loss 2.6593, train_accu: 0.9612, dev_accu: 0.8828\n",
      "Epoch 23: train_loss 2.5970, train_accu: 0.9621, dev_accu: 0.8826\n",
      "Epoch 24: train_loss 2.5384, train_accu: 0.9628, dev_accu: 0.8826\n"
     ]
    }
   ],
   "source": [
    "model_w2_feat_tune = FeedForwardTagger(window_size=2, \n",
    "                                  output_dim=len(all_labels),\n",
    "                                       pretrained_emb=twitter_emb,\n",
    "                                 num_binary_features=len(FEATURES),\n",
    "                                 num_count_features=2)\n",
    "best_model_w2_feat_tune, df_w2_feat_tune = \\\n",
    "train_util(model_w2_feat_tune, X_train_w2_feat_pre, Y_train, X_dev_w2_feat_pre, Y_dev, \n",
    "                                  n_epochs=25, lr=0.5, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devtest_accu: 0.8916\n"
     ]
    }
   ],
   "source": [
    "devtest_preds = torch.argmax(best_model_w2_feat_tune(X_devtest_w2_feat_pre), dim=1)\n",
    "devtest_accu = accuracy_score(Y_devtest, devtest_preds)\n",
    "conf_matrix_w2_feat_tune = confusion_matrix(Y_devtest, devtest_preds)\n",
    "print('devtest_accu: {:.4f}'.format(devtest_accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
