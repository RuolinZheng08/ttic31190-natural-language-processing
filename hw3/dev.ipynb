{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/twpos-train.tsv') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = text.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“\\t,\\n<@MENTION>\\t@\\n:\\t,\\nnow\\tR\\nfollowing\\tV\\n<@MENTION>\\t@\\n”\\t,\\nlmao\\t!\\nare\\tV\\nyou\\tO\\nhigh\\tA\\n?\\t,'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines[0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i\\tO'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i\\tO',\n",
       " 'predict\\tV',\n",
       " 'i\\tO',\n",
       " \"won't\\tV\",\n",
       " 'win\\tV',\n",
       " 'a\\tD',\n",
       " 'single\\tA',\n",
       " 'game\\tN',\n",
       " 'i\\tO',\n",
       " 'bet\\tV',\n",
       " 'on\\tP',\n",
       " '.\\t,',\n",
       " 'got\\tV',\n",
       " 'cliff\\t^',\n",
       " 'lee\\t^',\n",
       " 'today\\tN',\n",
       " ',\\t,',\n",
       " 'so\\tP',\n",
       " 'if\\tP',\n",
       " 'he\\tO',\n",
       " 'loses\\tV',\n",
       " 'its\\tL',\n",
       " 'on\\tP',\n",
       " 'me\\tO',\n",
       " 'rt\\t~',\n",
       " '<@MENTION>\\t@',\n",
       " ':\\t~',\n",
       " 'texas\\t^',\n",
       " '(\\t,',\n",
       " 'cont\\t~',\n",
       " ')\\t,',\n",
       " '<URL-tl.gd>\\tU']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    with open(file, 'rt') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n\\n')\n",
    "    data = []\n",
    "    labels = []\n",
    "    vocab = set()\n",
    "    label_set = set()\n",
    "    for line in lines:\n",
    "        if not line: continue\n",
    "        curr_data = []\n",
    "        curr_labels = []\n",
    "        for token_label_str in line.split('\\n'):\n",
    "            if not token_label_str: continue\n",
    "            token, label = token_label_str.split('\\t')\n",
    "            vocab.add(token)\n",
    "            label_set.add(label)\n",
    "            curr_data.append(token)\n",
    "            curr_labels.append(label)\n",
    "        data.append(curr_data)\n",
    "        labels.append(curr_labels)\n",
    "    return data, labels, vocab, label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, train_vocab, train_label_set = read_corpus('data/twpos-train.tsv')\n",
    "dev, dev_labels, dev_vocab, dev_label_set = read_corpus('data/twpos-dev.tsv')\n",
    "devtest, devtest_labels, devtest_vocab, devtest_label_set = read_corpus('data/twpos-devtest.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_vocab.copy()\n",
    "vocab.update(dev_vocab)\n",
    "vocab.update(devtest_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = train_label_set.copy()\n",
    "label_set.update(dev_label_set)\n",
    "label_set.update(devtest_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1173, 327, 327)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(dev), len(devtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4420, 1750, 1705, 5989)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab), len(dev_vocab), len(devtest_vocab), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = sorted(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '$',\n",
       " '&',\n",
       " ',',\n",
       " '@',\n",
       " 'A',\n",
       " 'D',\n",
       " 'E',\n",
       " 'G',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '^',\n",
       " '~']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_map = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#drake',\n",
       " '#dui',\n",
       " '#dumb',\n",
       " '#dumbniggathere',\n",
       " '#ebay',\n",
       " '#econdev',\n",
       " '#education',\n",
       " '#elearning',\n",
       " '#endtimes',\n",
       " '#englishmajors']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word_map[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_map = {word: idx for idx, word in enumerate(idx2word_map)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2811"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx_map['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17130, 4821, 4639)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(l) for l in train]), sum([len(l) for l in dev]), \\\n",
    "sum([len(l) for l in devtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'predict',\n",
       " 'i',\n",
       " \"won't\",\n",
       " 'win',\n",
       " 'a',\n",
       " 'single',\n",
       " 'game',\n",
       " 'i',\n",
       " 'bet',\n",
       " 'on',\n",
       " '.',\n",
       " 'got',\n",
       " 'cliff',\n",
       " 'lee',\n",
       " 'today',\n",
       " ',',\n",
       " 'so',\n",
       " 'if',\n",
       " 'he',\n",
       " 'loses',\n",
       " 'its',\n",
       " 'on',\n",
       " 'me',\n",
       " 'rt',\n",
       " '<@MENTION>',\n",
       " ':',\n",
       " 'texas',\n",
       " '(',\n",
       " 'cont',\n",
       " ')',\n",
       " '<URL-tl.gd>']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'V',\n",
       " 'O',\n",
       " 'V',\n",
       " 'V',\n",
       " 'D',\n",
       " 'A',\n",
       " 'N',\n",
       " 'O',\n",
       " 'V',\n",
       " 'P',\n",
       " ',',\n",
       " 'V',\n",
       " '^',\n",
       " '^',\n",
       " 'N',\n",
       " ',',\n",
       " 'P',\n",
       " 'P',\n",
       " 'O',\n",
       " 'V',\n",
       " 'L',\n",
       " 'P',\n",
       " 'O',\n",
       " '~',\n",
       " '@',\n",
       " '~',\n",
       " '^',\n",
       " ',',\n",
       " '~',\n",
       " ',',\n",
       " 'U']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [[elm] for elm in train_labels[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(categories=[all_labels])\n",
    "mat = enc.fit_transform(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['!', '#', '$', '&', ',', '@', 'A', 'D', 'E', 'G', 'L', 'M', 'N',\n",
       "        'O', 'P', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z', '^', '~'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(mat.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, window_size, output_dim,\n",
    "                 emb_dim=50, pretrained_emb=None, freeze=False):\n",
    "        super(FeedForwardTagger, self).__init__()\n",
    "        if pretrained_emb:\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrain_emb)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        input_dim = (2 * window_size + 1) * emb_dim\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.emb(inputs).view((1, -1))\n",
    "        out = F.tanh(self.fc1(embeds))\n",
    "        out = self.fc2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> 0.008005 0.008839 -0.007661 -0.006556 0.002733 0.006042 0.001882 0.000423 -0.007207 0.004437 -0.008713 0.002499 -0.001503 -0.001914 -0.006631 -0.003764 0.005159 0.006051 0.005938 0.003195 0.003090 -0.007605 -0.008192 0.009939 0.007603 0.006180 -0.001208 0.008031 -0.000990 0.001469 -0.000298 -0.005966 0.002625 -0.002675 -0.007651 0.009508 0.008759 -0.002190 -0.000452 0.001018 -0.007275 -0.008014 0.009109 0.000126 -0.005165 -0.006084 -0.006153 0.003394 0.000403 0.002662\n",
      ". 0.207125 -0.031345 0.091379 0.096409 -0.131985 0.144753 0.026806 -0.175994 0.048449 0.081937 0.080954 -0.041135 -0.304135 0.055592 0.147729 -0.079038 0.295870 0.253811 -0.181798 0.248076 0.053270 0.209448 -0.219176 0.178508 0.102624 0.087713 0.233860 0.101264 0.040922 -0.127173 -0.236495 0.106080 -0.306207 0.138877 0.152549 0.144254 -0.044860 0.104159 0.041576 -0.202435 0.077845 -0.081174 -0.220780 -0.122013 -0.297300 0.247084 0.324700 0.155697 -0.067304 -0.025445\n",
      "<@MENTION> -0.321289 0.050717 0.217663 0.130636 0.220566 -0.221929 -0.009311 0.241410 0.420372 -0.318453 0.245157 -0.127106 0.050635 -0.093491 -0.284748 0.440062 0.277786 -0.009949 -0.752337 0.002765 0.156626 0.000701 -0.635457 0.163263 -0.572934 -0.481954 0.184239 -0.102899 0.382553 0.078491 -0.026412 -0.030684 0.153669 0.467496 -0.249059 0.302701 0.009777 -0.528866 0.467654 -0.494794 -0.196051 -0.650321 0.229833 -0.056008 -0.589202 0.037581 0.197169 -0.160581 0.057263 0.384165\n",
      "the 0.234545 0.005101 0.228226 0.180391 0.134255 0.097655 0.077292 0.102930 0.253679 -0.195276 0.256240 -0.358088 -0.414542 -0.221580 0.332130 -0.145090 0.334817 0.052972 -0.169803 0.539237 0.001061 -0.016713 0.112819 0.205732 0.015363 0.115949 0.352880 -0.021058 0.143703 0.244716 0.210240 -0.233965 -0.190863 0.314005 0.281896 0.315395 -0.006156 0.112726 -0.015829 0.092465 0.238178 -0.367623 0.009977 -0.245449 -0.156343 0.175603 -0.200755 0.089917 -0.050858 0.064103\n",
      "i -0.065632 -0.022718 0.058576 0.560731 0.154797 0.307549 0.063307 -0.363784 -0.133096 0.187719 0.284242 -0.169684 -0.344151 0.088089 0.012419 -0.071703 -0.409381 -0.116806 -0.326219 -0.209986 -0.016138 0.597445 -0.544442 -0.103491 -0.295425 -0.184095 0.156823 0.052815 0.087674 0.015464 0.450288 -0.427743 0.138449 -0.040314 -0.032530 0.004041 -0.331920 0.147574 0.028140 -0.338613 -0.482019 -0.531158 -0.545102 0.219512 -0.231775 0.191794 0.101574 -0.262040 0.026996 -0.015891\n",
      ", 0.092552 0.055209 -0.002700 0.159089 0.229496 0.083656 0.264748 -0.073015 -0.086687 0.065802 0.070484 -0.078595 -0.241872 0.087684 0.203424 -0.020434 0.127434 0.099769 0.096219 0.115561 0.060562 0.146752 -0.034801 0.331189 -0.110506 0.030618 0.251049 -0.030396 0.077863 -0.299788 -0.307094 -0.075314 -0.047092 0.044168 0.185137 0.102289 -0.157833 0.081993 0.065105 0.034936 0.056609 -0.116558 -0.105299 -0.095515 -0.330808 0.035904 0.184974 0.310487 0.207457 -0.041662\n",
      "to 0.262741 0.264479 0.144042 0.393530 0.236903 0.524451 0.023663 -0.271532 -0.233795 0.030010 0.161722 -0.223735 -0.267646 -0.100555 0.054112 0.015891 0.422145 -0.171851 -0.020472 0.193208 0.221960 0.005816 -0.136133 -0.366301 -0.307279 -0.037068 0.097324 0.233212 0.430246 0.266055 -0.165139 0.091811 0.059727 0.080356 -0.125819 -0.142177 0.028436 0.033472 -0.306612 -0.219535 0.314914 -0.342489 -0.040743 -0.002987 -0.193411 -0.109001 0.036950 0.300151 -0.114113 -0.241513\n",
      ": -0.126286 -0.000078 0.239206 0.019466 0.006305 -0.205536 0.138923 0.243137 0.378534 -0.061952 0.438600 -0.150990 0.207042 0.029422 -0.222612 0.588875 0.145958 -0.121144 -0.130677 0.285464 0.242902 0.070870 -0.538069 0.302510 -0.261985 -0.077360 0.013149 0.041560 0.519098 0.066713 -0.130469 -0.202223 -0.140710 0.236506 -0.057964 0.472518 0.190818 -0.351139 0.301636 -0.436348 -0.018925 -0.356319 0.216191 0.004838 -0.183736 -0.015686 -0.229088 0.269262 0.082155 -0.225666\n",
      "a 0.116213 0.125402 0.307476 0.274222 0.467254 -0.422797 0.244325 -0.076489 0.123519 -0.129322 0.061179 -0.450824 -0.212051 -0.084627 0.041500 0.147137 -0.149689 -0.074571 -0.539109 0.509563 0.022619 0.284628 0.142740 -0.237212 0.268584 0.175040 0.392077 -0.057690 0.414145 0.095697 -0.228929 0.058631 0.078697 -0.003131 0.146591 -0.273123 -0.183319 0.027113 -0.308410 -0.342612 0.191522 -0.100600 -0.327522 -0.055878 -0.173632 0.299825 -0.107830 0.053188 -0.026189 0.139024\n",
      "! -0.178815 -0.168813 0.173222 0.077957 -0.160614 -0.037057 0.051117 0.029850 0.039200 -0.034840 -0.234721 0.107821 -0.069225 -0.124002 -0.092150 -0.095564 0.290540 0.332429 -0.320220 0.000218 0.016866 0.179454 -0.273376 0.034743 0.057511 -0.027645 0.072842 0.215208 0.158244 -0.055194 -0.093649 0.344081 -0.060768 0.324838 0.370267 0.151149 -0.048027 0.067422 -0.002119 -0.138948 -0.015382 0.009545 -0.115081 0.167323 -0.557406 0.127479 0.104625 -0.069702 0.022322 -0.112278\n",
      "you -0.038781 0.304098 -0.040077 0.356578 0.021050 -0.126812 0.085686 -0.315427 -0.188337 0.270221 0.074642 0.060128 -0.263897 -0.005955 0.396528 -0.229166 0.073427 0.308674 -0.313733 0.107063 0.144635 0.220842 -0.142435 -0.199898 -0.172050 -0.173546 0.248014 -0.100993 -0.116605 -0.128280 0.040588 -0.375462 0.052865 0.180489 0.056199 -0.169289 -0.127000 -0.103025 0.001308 -0.259215 -0.143454 -0.754082 -0.549543 0.249102 -0.338669 0.166132 -0.030713 0.148149 0.068383 0.022215\n",
      "and 0.139175 0.140427 -0.036599 0.029156 0.139191 0.068358 0.169610 -0.276871 -0.237193 -0.043509 0.184377 -0.221820 -0.223036 0.010016 0.057851 0.082399 0.220361 0.048646 0.058384 0.012763 0.123364 0.023525 -0.176440 0.129477 0.076030 -0.049190 0.161203 0.130932 0.170234 -0.201939 -0.171198 -0.124815 -0.037113 0.070437 0.274445 0.070899 0.033529 0.235625 -0.012286 -0.043769 0.023937 -0.159832 0.032283 -0.192547 -0.240468 0.109552 0.189568 0.192779 0.102152 0.004908\n",
      "... 0.112889 -0.224176 0.285656 -0.035101 0.061001 -0.100191 0.032661 -0.087020 0.018436 0.161328 -0.048883 0.150996 -0.026743 -0.282437 0.085597 0.013046 0.275531 0.099538 0.181672 0.174818 -0.147428 0.355127 -0.377915 -0.054945 0.293248 -0.207147 0.124111 -0.160809 -0.093124 0.113528 -0.337195 0.205752 0.009230 0.325895 0.166417 0.121005 -0.095728 -0.116095 0.074169 -0.197984 -0.158288 -0.029886 -0.270006 -0.160808 -0.259067 0.189530 0.261233 0.099761 0.005992 -0.050428\n",
      "is 0.031805 0.025970 -0.096576 0.193760 0.101659 0.045159 0.035515 0.331388 -0.413051 -0.030400 0.048439 0.349465 -0.060777 -0.276686 0.244521 0.030281 0.184632 0.007042 -0.520919 0.457175 -0.248058 0.150038 -0.303639 0.355448 -0.047626 -0.020561 0.221093 0.226479 0.308150 0.174563 -0.213116 -0.173703 -0.085129 -0.185593 -0.062478 -0.061129 -0.157959 0.199148 0.135979 -0.088852 0.206692 -0.465602 -0.137727 -0.204555 -0.092413 0.158989 0.276796 -0.100220 0.248639 -0.464332\n",
      "rt -0.499217 -0.166155 0.090818 0.102002 0.120636 -0.521082 -0.267535 0.174037 0.556078 -0.029390 0.272770 0.016905 0.042527 -0.293638 -0.788151 0.064098 0.260393 0.767683 -0.394788 -0.221667 -0.145077 -0.316341 -0.193266 0.149395 -0.367065 -0.327196 0.142788 0.031715 0.596779 -0.053770 -0.332626 0.252753 -0.096113 -0.225355 0.104059 0.052368 -0.135418 -0.274321 0.090823 -0.535762 -0.322226 -0.383325 0.004407 0.850419 -0.523188 -0.139698 0.221676 0.059167 -0.111941 0.075055\n",
      "in 0.209979 -0.116985 -0.113077 -0.110672 0.263573 0.165238 0.032019 0.107845 -0.171875 0.046378 0.200731 -0.136005 -0.416787 -0.070951 0.020999 -0.239079 0.197097 -0.050397 0.004749 0.114340 -0.298717 0.205080 -0.175142 0.135654 0.050222 0.044652 0.294726 0.157812 -0.016998 -0.155698 -0.307053 -0.083630 0.226107 0.070527 -0.040786 0.079794 0.199380 0.162901 -0.109671 -0.332905 0.164909 -0.214920 0.069301 0.009089 -0.343648 0.192014 -0.284581 0.029479 -0.010443 -0.177145\n",
      "of -0.120889 0.065212 -0.025792 -0.165523 0.277096 0.073093 -0.115059 -0.253179 0.117577 0.178600 0.524558 -0.061898 -0.366696 -0.286378 -0.078306 -0.167653 -0.007201 -0.162928 -0.027723 0.512694 -0.152462 -0.035071 -0.155276 0.150445 0.108315 -0.022299 0.328748 0.082482 -0.056417 -0.413198 0.240334 0.038600 0.050487 0.171549 -0.321537 0.064454 -0.054980 0.418330 -0.198806 0.020486 0.337998 -0.100049 0.323793 -0.115668 -0.344771 0.142604 -0.021716 0.040408 0.025916 -0.491374\n",
      "my 0.317494 0.057408 0.078970 0.020125 0.660932 -0.232432 0.059059 0.082704 -0.067710 -0.377684 0.081525 -0.037372 -0.301996 -0.134179 -0.165355 -0.182054 0.153324 -0.107798 -0.280596 0.224507 0.138119 0.431389 -0.309744 -0.064140 0.183254 -0.072924 0.198896 0.291184 0.029168 0.249821 0.306935 -0.225569 0.038462 0.169157 0.595905 -0.015372 -0.036182 0.353838 0.147169 -0.058698 -0.124059 -0.275618 -0.197332 0.064310 -0.315278 -0.203695 0.142165 0.170725 -0.112047 0.340214\n",
      "for 0.105866 0.087160 0.128572 -0.185238 -0.022962 0.037489 0.104209 0.043736 -0.171829 -0.023640 0.118504 -0.053359 -0.197304 -0.108694 0.023203 0.049425 0.133023 0.077675 0.024178 -0.102849 0.059739 -0.089063 -0.134969 -0.142332 0.092754 -0.009948 0.218974 0.444965 0.209571 -0.011157 -0.241487 0.012485 0.227832 0.202579 -0.146327 0.044288 -0.107359 0.095669 -0.051305 -0.070038 0.262947 -0.140833 -0.123144 -0.115084 -0.357919 0.086645 0.236072 -0.047017 0.321753 -0.289371\n",
      "? 0.145969 -0.103202 0.200870 -0.055812 -0.148082 -0.212714 0.100013 -0.018726 -0.060423 0.231069 -0.299043 0.282192 0.163264 -0.193566 -0.127857 -0.106074 0.357866 0.228942 -0.192680 0.236026 -0.135338 0.068645 -0.321671 -0.095506 -0.054281 0.010124 0.324809 -0.229094 0.223562 -0.020456 -0.372743 -0.045112 -0.150886 0.391709 0.218171 0.129307 0.155823 0.169678 0.104494 -0.268282 0.147108 -0.133661 -0.164606 0.320339 -0.205442 -0.025547 0.181508 -0.101619 0.125902 -0.175240\n"
     ]
    }
   ],
   "source": [
    "!head -n20 data/twitter-embeddings.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = pd.read_csv('data/twitter-embeddings.txt', delimiter=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.008839</td>\n",
       "      <td>-0.007661</td>\n",
       "      <td>-0.006556</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>-0.007207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007275</td>\n",
       "      <td>-0.008014</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.006084</td>\n",
       "      <td>-0.006153</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.002662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>0.207125</td>\n",
       "      <td>-0.031345</td>\n",
       "      <td>0.091379</td>\n",
       "      <td>0.096409</td>\n",
       "      <td>-0.131985</td>\n",
       "      <td>0.144753</td>\n",
       "      <td>0.026806</td>\n",
       "      <td>-0.175994</td>\n",
       "      <td>0.048449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077845</td>\n",
       "      <td>-0.081174</td>\n",
       "      <td>-0.220780</td>\n",
       "      <td>-0.122013</td>\n",
       "      <td>-0.297300</td>\n",
       "      <td>0.247084</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.155697</td>\n",
       "      <td>-0.067304</td>\n",
       "      <td>-0.025445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;@MENTION&gt;</td>\n",
       "      <td>-0.321289</td>\n",
       "      <td>0.050717</td>\n",
       "      <td>0.217663</td>\n",
       "      <td>0.130636</td>\n",
       "      <td>0.220566</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>0.241410</td>\n",
       "      <td>0.420372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196051</td>\n",
       "      <td>-0.650321</td>\n",
       "      <td>0.229833</td>\n",
       "      <td>-0.056008</td>\n",
       "      <td>-0.589202</td>\n",
       "      <td>0.037581</td>\n",
       "      <td>0.197169</td>\n",
       "      <td>-0.160581</td>\n",
       "      <td>0.057263</td>\n",
       "      <td>0.384165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>0.234545</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.228226</td>\n",
       "      <td>0.180391</td>\n",
       "      <td>0.134255</td>\n",
       "      <td>0.097655</td>\n",
       "      <td>0.077292</td>\n",
       "      <td>0.102930</td>\n",
       "      <td>0.253679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238178</td>\n",
       "      <td>-0.367623</td>\n",
       "      <td>0.009977</td>\n",
       "      <td>-0.245449</td>\n",
       "      <td>-0.156343</td>\n",
       "      <td>0.175603</td>\n",
       "      <td>-0.200755</td>\n",
       "      <td>0.089917</td>\n",
       "      <td>-0.050858</td>\n",
       "      <td>0.064103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i</td>\n",
       "      <td>-0.065632</td>\n",
       "      <td>-0.022718</td>\n",
       "      <td>0.058576</td>\n",
       "      <td>0.560731</td>\n",
       "      <td>0.154797</td>\n",
       "      <td>0.307549</td>\n",
       "      <td>0.063307</td>\n",
       "      <td>-0.363784</td>\n",
       "      <td>-0.133096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482019</td>\n",
       "      <td>-0.531158</td>\n",
       "      <td>-0.545102</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>-0.231775</td>\n",
       "      <td>0.191794</td>\n",
       "      <td>0.101574</td>\n",
       "      <td>-0.262040</td>\n",
       "      <td>0.026996</td>\n",
       "      <td>-0.015891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19439</th>\n",
       "      <td>nurture</td>\n",
       "      <td>0.204274</td>\n",
       "      <td>0.164465</td>\n",
       "      <td>-0.109088</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>-0.017280</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>0.306388</td>\n",
       "      <td>-0.110153</td>\n",
       "      <td>-0.409500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151075</td>\n",
       "      <td>-0.173320</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>-0.057272</td>\n",
       "      <td>-0.667943</td>\n",
       "      <td>-0.516890</td>\n",
       "      <td>0.430046</td>\n",
       "      <td>0.418442</td>\n",
       "      <td>-0.074517</td>\n",
       "      <td>-0.217384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19440</th>\n",
       "      <td>quotation</td>\n",
       "      <td>-0.052569</td>\n",
       "      <td>-0.284374</td>\n",
       "      <td>0.339601</td>\n",
       "      <td>-0.435571</td>\n",
       "      <td>-0.337225</td>\n",
       "      <td>-0.354862</td>\n",
       "      <td>0.075978</td>\n",
       "      <td>-0.327963</td>\n",
       "      <td>0.094719</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077608</td>\n",
       "      <td>-0.066172</td>\n",
       "      <td>0.125516</td>\n",
       "      <td>-0.159479</td>\n",
       "      <td>0.430853</td>\n",
       "      <td>-0.173255</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.732749</td>\n",
       "      <td>-0.097984</td>\n",
       "      <td>-0.392376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19441</th>\n",
       "      <td>a5</td>\n",
       "      <td>-0.861113</td>\n",
       "      <td>0.340245</td>\n",
       "      <td>0.160136</td>\n",
       "      <td>-0.285174</td>\n",
       "      <td>0.249593</td>\n",
       "      <td>-0.123985</td>\n",
       "      <td>0.155226</td>\n",
       "      <td>0.512830</td>\n",
       "      <td>-0.140564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215618</td>\n",
       "      <td>0.437744</td>\n",
       "      <td>0.396084</td>\n",
       "      <td>0.076783</td>\n",
       "      <td>-0.034082</td>\n",
       "      <td>0.142286</td>\n",
       "      <td>-0.148950</td>\n",
       "      <td>0.401117</td>\n",
       "      <td>-0.371847</td>\n",
       "      <td>-0.407417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19442</th>\n",
       "      <td>malu</td>\n",
       "      <td>-0.105215</td>\n",
       "      <td>-0.278582</td>\n",
       "      <td>0.499606</td>\n",
       "      <td>-0.240102</td>\n",
       "      <td>0.115366</td>\n",
       "      <td>-0.423914</td>\n",
       "      <td>-0.456027</td>\n",
       "      <td>0.434088</td>\n",
       "      <td>0.235833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445639</td>\n",
       "      <td>-0.095685</td>\n",
       "      <td>0.306776</td>\n",
       "      <td>0.139477</td>\n",
       "      <td>-0.130772</td>\n",
       "      <td>-0.350481</td>\n",
       "      <td>0.445511</td>\n",
       "      <td>-0.255516</td>\n",
       "      <td>-0.221685</td>\n",
       "      <td>0.517086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19443</th>\n",
       "      <td>UUUNKKK</td>\n",
       "      <td>-0.238291</td>\n",
       "      <td>0.051810</td>\n",
       "      <td>0.338045</td>\n",
       "      <td>-0.053179</td>\n",
       "      <td>-0.087980</td>\n",
       "      <td>0.091420</td>\n",
       "      <td>-0.032903</td>\n",
       "      <td>0.195767</td>\n",
       "      <td>-0.044244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091530</td>\n",
       "      <td>0.025527</td>\n",
       "      <td>0.210625</td>\n",
       "      <td>0.104794</td>\n",
       "      <td>-0.074413</td>\n",
       "      <td>-0.051765</td>\n",
       "      <td>0.123640</td>\n",
       "      <td>0.177218</td>\n",
       "      <td>-0.060529</td>\n",
       "      <td>-0.163894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19444 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6   \\\n",
       "0            </s>  0.008005  0.008839 -0.007661 -0.006556  0.002733  0.006042   \n",
       "1               .  0.207125 -0.031345  0.091379  0.096409 -0.131985  0.144753   \n",
       "2      <@MENTION> -0.321289  0.050717  0.217663  0.130636  0.220566 -0.221929   \n",
       "3             the  0.234545  0.005101  0.228226  0.180391  0.134255  0.097655   \n",
       "4               i -0.065632 -0.022718  0.058576  0.560731  0.154797  0.307549   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "19439     nurture  0.204274  0.164465 -0.109088  0.008565 -0.017280  0.128900   \n",
       "19440   quotation -0.052569 -0.284374  0.339601 -0.435571 -0.337225 -0.354862   \n",
       "19441          a5 -0.861113  0.340245  0.160136 -0.285174  0.249593 -0.123985   \n",
       "19442        malu -0.105215 -0.278582  0.499606 -0.240102  0.115366 -0.423914   \n",
       "19443     UUUNKKK -0.238291  0.051810  0.338045 -0.053179 -0.087980  0.091420   \n",
       "\n",
       "             7         8         9   ...        41        42        43  \\\n",
       "0      0.001882  0.000423 -0.007207  ... -0.007275 -0.008014  0.009109   \n",
       "1      0.026806 -0.175994  0.048449  ...  0.077845 -0.081174 -0.220780   \n",
       "2     -0.009311  0.241410  0.420372  ... -0.196051 -0.650321  0.229833   \n",
       "3      0.077292  0.102930  0.253679  ...  0.238178 -0.367623  0.009977   \n",
       "4      0.063307 -0.363784 -0.133096  ... -0.482019 -0.531158 -0.545102   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "19439  0.306388 -0.110153 -0.409500  ...  0.151075 -0.173320  0.169558   \n",
       "19440  0.075978 -0.327963  0.094719  ... -0.077608 -0.066172  0.125516   \n",
       "19441  0.155226  0.512830 -0.140564  ...  0.215618  0.437744  0.396084   \n",
       "19442 -0.456027  0.434088  0.235833  ... -0.445639 -0.095685  0.306776   \n",
       "19443 -0.032903  0.195767 -0.044244  ... -0.091530  0.025527  0.210625   \n",
       "\n",
       "             44        45        46        47        48        49        50  \n",
       "0      0.000126 -0.005165 -0.006084 -0.006153  0.003394  0.000403  0.002662  \n",
       "1     -0.122013 -0.297300  0.247084  0.324700  0.155697 -0.067304 -0.025445  \n",
       "2     -0.056008 -0.589202  0.037581  0.197169 -0.160581  0.057263  0.384165  \n",
       "3     -0.245449 -0.156343  0.175603 -0.200755  0.089917 -0.050858  0.064103  \n",
       "4      0.219512 -0.231775  0.191794  0.101574 -0.262040  0.026996 -0.015891  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19439 -0.057272 -0.667943 -0.516890  0.430046  0.418442 -0.074517 -0.217384  \n",
       "19440 -0.159479  0.430853 -0.173255  0.292680  0.732749 -0.097984 -0.392376  \n",
       "19441  0.076783 -0.034082  0.142286 -0.148950  0.401117 -0.371847 -0.407417  \n",
       "19442  0.139477 -0.130772 -0.350481  0.445511 -0.255516 -0.221685  0.517086  \n",
       "19443  0.104794 -0.074413 -0.051765  0.123640  0.177218 -0.060529 -0.163894  \n",
       "\n",
       "[19444 rows x 51 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
