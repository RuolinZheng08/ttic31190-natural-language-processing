{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5ebbf70eb0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use slightly different utility functions from feed-forward tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        lines: [['hello', 'world'], ...]\n",
    "        labels: [[!, N], ...], a list of variable len tensors\n",
    "        vocab, all_labels\n",
    "    \"\"\"\n",
    "    with open(file, 'rt') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n\\n')\n",
    "    ret_lines = []\n",
    "    ret_labels = []\n",
    "    vocab = set()\n",
    "    all_labels = set()\n",
    "    for line in lines:\n",
    "        if not line: \n",
    "            continue\n",
    "        curr_line = []\n",
    "        curr_line_labels = []\n",
    "        for token_label_str in line.split('\\n'):\n",
    "            if not token_label_str: \n",
    "                continue\n",
    "            token, label = token_label_str.split('\\t')\n",
    "            vocab.add(token)\n",
    "            all_labels.add(label)\n",
    "            curr_line_labels.append(label)\n",
    "            curr_line.append(token)\n",
    "            \n",
    "        ret_labels.append(curr_line_labels)\n",
    "        ret_lines.append(curr_line)\n",
    "        \n",
    "    return ret_lines, ret_labels, vocab, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lines(lines, word2idx_map):\n",
    "    \"\"\"\n",
    "    returns a list of len(line) x 1 tensors\n",
    "    a list of variable length tensor for pad_sequence\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for line in lines:\n",
    "        encoded_line = []\n",
    "        for word in line:\n",
    "            num = word2idx_map.get(word, word2idx_map['UUUNKKK'])\n",
    "            encoded_line.append(num)\n",
    "        ret.append(torch.tensor(encoded_line))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(line_labels, encoder):\n",
    "    \"\"\"\n",
    "    returns a list of len(line) x 1 tensors\n",
    "    a list of variable length tensor for pad_sequence\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for line_label in line_labels:\n",
    "        encoded = encoder.transform(line_label)\n",
    "        ret.append(torch.tensor(encoded, dtype=torch.long))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTagger(nn.Module):\n",
    "    def __init__(self, pretrained_embedding, output_dim, rnn_layer_func, \n",
    "                 bidirectional=False):\n",
    "        \"\"\"\n",
    "        rnn_layer_func could be nn.RNN, nn.GRU, or nn.LSTM\n",
    "        output_dim is the number of tag classes\n",
    "        use 128 for hidden\n",
    "        \"\"\"\n",
    "        super(Seq2SeqTagger, self).__init__()\n",
    "    \n",
    "        input_dim = pretrained_embedding.shape[1]\n",
    "        self.hidden_dim = 128\n",
    "        self.emb = nn.Embedding.from_pretrained(pretrained_embedding)\n",
    "        \n",
    "        self.rnn = rnn_layer_func(input_dim, hidden_size=self.hidden_dim, \n",
    "                                  bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.emb(inputs)\n",
    "        hidden = torch.zeros(1, x.shape[1], self.hidden_dim)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        log_probs = F.log_softmax(out, dim=2)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util(model, X_train, Y_train, X_dev, Y_dev, n_epochs, lr, \n",
    "              batch_size):\n",
    "    \"\"\"\n",
    "    returns: best_model, losses, train_accu_list, dev_accu_list\n",
    "    \"\"\"\n",
    "    # for accuracy_score\n",
    "    Y_train_1d = Y_train.flatten()\n",
    "    Y_dev_1d = Y_dev.flatten()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_model = None\n",
    "    best_dev_accu = 0\n",
    "    losses = []\n",
    "    train_accu_list, dev_accu_list = [], []\n",
    "    \n",
    "    progress_bar = tqdm.trange(1, n_epochs + 1)\n",
    "    for epoch in progress_bar:\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(X_train[i : i + batch_size])\n",
    "            # reshape log_probs and labels\n",
    "            log_probs_2d = log_probs.view(\n",
    "                log_probs.shape[0] * log_probs.shape[1], -1)\n",
    "            targets = Y_train[i : i + batch_size]\n",
    "            targets_1d = targets.view(targets.shape[0] * targets.shape[1]) \n",
    "           \n",
    "            loss = loss_func(log_probs_2d, targets_1d)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_preds = torch.argmax(model(X_train), dim=2).flatten()\n",
    "        train_accu = accuracy_score(Y_train_1d, train_preds)\n",
    "        # evaluate on dev\n",
    "        dev_preds = torch.argmax(model(X_dev), dim=2).flatten()\n",
    "        dev_accu = accuracy_score(Y_dev_1d, dev_preds)\n",
    "        \n",
    "        # early stopping, save the model if it has improved on dev\n",
    "        if dev_accu > best_dev_accu:\n",
    "            best_dev_accu = dev_accu\n",
    "            best_model = deepcopy(model)\n",
    "        \n",
    "        progress_bar.set_description(\n",
    "            'Epoch {}: train_loss {:.4f}, train_accu: {:.4f}, dev_accu: {:.4f}'\\\n",
    "              .format(epoch, epoch_loss, train_accu, dev_accu))\n",
    "        losses.append(epoch_loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        dev_accu_list.append(dev_accu)\n",
    "        \n",
    "    loss_accu_df = pd.DataFrame({\n",
    "        'epoch': range(1, n_epochs + 1), \n",
    "        'loss': losses,\n",
    "        'train_accu': train_accu_list,\n",
    "        'dev_accu': dev_accu_list})\n",
    "        \n",
    "    return best_model, loss_accu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accu(loss_accu_df_list, window_list):\n",
    "    \"\"\"\n",
    "    input: two lists of the same length, loss_accu_df, window\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for df, w in zip(loss_accu_df_list, window_list):\n",
    "        df1 = df.melt(\n",
    "            'epoch', value_vars=['loss']).assign(window=w, plot='loss')\n",
    "        df2 = df.melt(\n",
    "            'epoch', value_vars=['train_accu', 'dev_accu']).assign(window=w, plot='accu')\n",
    "        dfs.extend([df1, df2])\n",
    "    plot_df = pd.concat(dfs)\n",
    "\n",
    "    g = sns.FacetGrid(data=plot_df, row='plot', col='window', \n",
    "                      hue='variable', sharey=False)\n",
    "    g.map_dataframe(sns.lineplot, x='epoch', y='value')\n",
    "    g.add_legend()\n",
    "\n",
    "def plot_confusion_matrix(matrix, labels, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, \n",
    "                     annot=True, fmt='d', cmap='Blues')\n",
    "    ax.set_xlabel('Predictions')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_vocab = []\n",
    "twitter_emb = []\n",
    "with open(DATADIR + 'twitter-embeddings.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split(' ')\n",
    "        word, emb = tokens[0], tokens[1:]\n",
    "        emb = [float(elm) for elm in emb]\n",
    "        twitter_vocab.append(word)\n",
    "        twitter_emb.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_emb = torch.tensor(twitter_emb)\n",
    "# for <s>, use the emb for </s>\n",
    "idx2word_pretrained = twitter_vocab + ['<s>']\n",
    "# construct maps for pretrained word embs\n",
    "word2idx_pretrained = {word: idx for idx, word in enumerate(idx2word_pretrained)}\n",
    "temp = twitter_emb[word2idx_pretrained['</s>']].view((1, -1))\n",
    "twitter_emb = torch.cat((twitter_emb, temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, train_vocab, all_labels = read_corpus(DATADIR + 'twpos-train.tsv')\n",
    "dev, dev_labels, dev_vocab, _ = read_corpus(DATADIR + 'twpos-dev.tsv')\n",
    "devtest, devtest_labels, devtest_vocab, all_labels_devtest = read_corpus(DATADIR + 'twpos-devtest.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '!', '#', '$', '&', ',', '@', 'A', 'D', 'E', 'G', 'L', 'M',\n",
       "       'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z', '^', '~'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to zero-pad lines and labels\n",
    "# add empty label before encoding all labels\n",
    "all_labels.add('')\n",
    "all_labels = np.array(list(all_labels))\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode and Zero-pad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode_lines(train, word2idx_pretrained)\n",
    "X_dev = encode_lines(dev, word2idx_pretrained)\n",
    "X_devtest = encode_lines(devtest, word2idx_pretrained)\n",
    "Y_train = encode_labels(train_labels, label_encoder)\n",
    "Y_dev = encode_labels(dev_labels, label_encoder)\n",
    "Y_devtest = encode_labels(devtest_labels, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad train, dev, devtest to same input_dim\n",
    "# (num_samples, input_dim)\n",
    "X_padded = pad_sequence([*X_train, *X_dev, *X_devtest], batch_first=True)\n",
    "Y_padded = pad_sequence([*Y_train, *Y_dev, *Y_devtest], batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1827, 38]), 1173, 327, 327)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded.shape, len(X_train), len(X_dev), len(X_devtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = X_padded[:len(X_train)]\n",
    "X_dev_padded = X_padded[len(X_train) : len(X_train) + len(X_dev)]\n",
    "X_devtest_padded = X_padded[-len(X_devtest):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1173, 38]), torch.Size([327, 38]), torch.Size([327, 38]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape, X_dev_padded.shape, X_devtest_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_padded = Y_padded[:len(Y_train)]\n",
    "Y_dev_padded = Y_padded[len(Y_train) : len(Y_train) + len(Y_dev)]\n",
    "Y_devtest_padded = Y_padded[-len(Y_devtest):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss 8.0948, train_accu: 0.6158, dev_accu: 0.6121:   4%|â–         | 1/25 [00:00<00:20,  1.19it/s]"
     ]
    }
   ],
   "source": [
    "model_rnn = Seq2SeqTagger(\n",
    "    pretrained_embedding=twitter_emb, \n",
    "    output_dim=all_labels.size, rnn_layer_func=nn.RNN)\n",
    "train_util(model_rnn, X_train_padded, Y_train_padded,\n",
    "          X_dev_padded, Y_dev_padded, n_epochs=25, lr=0.5, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
