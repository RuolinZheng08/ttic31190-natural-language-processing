{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        lines: [['hello', 'world'], ...]\n",
    "        labels: [[!], [N], ...]\n",
    "        vocab\n",
    "    \"\"\"\n",
    "    with open(file, 'rt') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n\\n')\n",
    "    ret_lines = []\n",
    "    labels = []\n",
    "    vocab = set()\n",
    "    for line in lines:\n",
    "        if not line: \n",
    "            continue\n",
    "        curr_line = []\n",
    "        for token_label_str in line.split('\\n'):\n",
    "            if not token_label_str: \n",
    "                continue\n",
    "            token, label = token_label_str.split('\\t')\n",
    "            vocab.add(token)\n",
    "            labels.append(label)\n",
    "            curr_line.append(token)\n",
    "        ret_lines.append(curr_line)\n",
    "    return ret_lines, labels, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lines(lines, word2idx_map, window_size):\n",
    "    \"\"\"\n",
    "    returns X: len(lines) x (2 * window_size + 1)\n",
    "    \"\"\"\n",
    "    def encode_line(line, word2idx_map, window_size):\n",
    "        num_repr = [] # numerical representation\n",
    "        for word in line:\n",
    "            num = word2idx_map.get(word, word2idx_map['UUUNKKK'])\n",
    "            num_repr.append(num)\n",
    "        # pad with start and end tokens\n",
    "        start = [word2idx_map['<s>']] * window_size\n",
    "        end = [word2idx_map['</s>']] * window_size\n",
    "        padded = start + num_repr + end\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(window_size, len(padded) - window_size):\n",
    "            windowed = padded[i - window_size : i + window_size + 1]\n",
    "            ret.append(windowed)\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    res = []\n",
    "    for line in lines:\n",
    "        res.extend(encode_line(line, word2idx_map, window_size))\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, output_dim,\n",
    "                 emb_dim=50, pretrained_emb=None, freeze=False):\n",
    "        super(FeedForwardTagger, self).__init__()\n",
    "        if pretrained_emb:\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrain_emb)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        input_dim = (2 * window_size + 1) * emb_dim\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.emb(inputs).squeeze()\n",
    "        out = F.tanh(self.fc1(embeds))\n",
    "        out = self.fc2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util(model, X_train, Y_train, X_dev, Y_dev, n_epochs):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=5)\n",
    "    \n",
    "    best_model = None\n",
    "    losses = []\n",
    "    train_accu_list, dev_accu_list = [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(X_train)\n",
    "        loss = loss_func(log_probs, Y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_accu = accuracy_score(Y_train, \n",
    "                                    torch.argmax(log_probs, dim=1))\n",
    "        # evaluate on dev\n",
    "        dev_preds = torch.argmax(model(X_dev), dim=1)\n",
    "        dev_accu = accuracy_score(Y_dev, dev_preds)\n",
    "        \n",
    "        print(epoch, loss.item(), train_accu, dev_accu)\n",
    "        losses.append(loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        dev_accu_list.append(dev_accu)\n",
    "    return losses, train_accu_list, dev_accu_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, train_vocab = read_corpus(DATADIR + 'twpos-train.tsv')\n",
    "dev, dev_labels, dev_vocab = read_corpus(DATADIR + 'twpos-dev.tsv')\n",
    "devtest, devtest_labels, devtest_vocab = read_corpus(DATADIR + 'twpos-devtest.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(set(train_labels)))\n",
    "Y_train = label_encoder.transform(train_labels)\n",
    "Y_dev = label_encoder.transform(dev_labels)\n",
    "Y_devtest = label_encoder.transform(devtest_labels)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_dev = torch.tensor(Y_dev, dtype=torch.long)\n",
    "Y_devtest = torch.tensor(Y_devtest, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_vocab.copy()\n",
    "vocab.update(dev_vocab)\n",
    "vocab.update(devtest_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline w/ Randomly Initialized Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for randomly initialized embs\n",
    "idx2word_rand = sorted(vocab)\n",
    "idx2word_rand += ['<s>', '</s>', 'UUUNKKK']\n",
    "word2idx_rand = {word: idx for idx, word in enumerate(idx2word_rand)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, DevTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0 = encode_lines(train, word2idx_rand, window_size=0)\n",
    "X_dev_w0 = encode_lines(dev, word2idx_rand, window_size=0)\n",
    "X_devtest_w0 = encode_lines(devtest, word2idx_rand, window_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 1\n",
    "X_train_w3 = encode_lines(train, word2idx_rand, window_size=1)\n",
    "X_dev_w3 = encode_lines(dev, word2idx_rand, window_size=1)\n",
    "X_devtest_w3 = encode_lines(devtest, word2idx_rand, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.2219648361206055 0.0655575014594279 0.3437046255963493\n",
      "1 2.2807838916778564 0.34740221833041446 0.4517734909769757\n",
      "2 1.9778130054473877 0.45125510799766494 0.45384774942957895\n",
      "3 1.8543860912322998 0.46468184471687096 0.4781165733250363\n",
      "4 1.7962263822555542 0.48488032691185057 0.46069280232316945\n",
      "5 1.8289300203323364 0.4687098657326328 0.4706492428956648\n",
      "6 1.8158886432647705 0.4692936368943374 0.39742791951877204\n",
      "7 2.0002593994140625 0.40758902510215994 0.4463804190002074\n",
      "8 1.904037594795227 0.43887915936952715 0.46608587429993775\n",
      "9 1.82635498046875 0.47046117921774666 0.5121344119477287\n",
      "10 1.7450987100601196 0.5076474022183304 0.5086081725783033\n",
      "11 1.6692945957183838 0.508990075890251 0.5193943165318399\n",
      "12 1.6115907430648804 0.5213076474022184 0.5098527276498652\n",
      "13 1.6431019306182861 0.512784588441331 0.527276498651732\n",
      "14 1.6350911855697632 0.5267367192060712 0.536403235843186\n",
      "15 1.5908668041229248 0.5405137186223 0.5420037336652147\n",
      "16 1.5037978887557983 0.5425569176882662 0.5486413607135449\n",
      "17 1.5017746686935425 0.5603035610040864 0.5621240406554656\n",
      "18 1.451823353767395 0.5652072387624051 0.551337896701929\n",
      "19 1.4665721654891968 0.5642732049036777 0.5706285003111388\n",
      "20 1.4643304347991943 0.5741389375364857 0.562953744036507\n",
      "21 1.4153668880462646 0.5784004670169294 0.5789255341215516\n",
      "22 1.3673757314682007 0.5817863397548161 0.5675171126322339\n",
      "23 1.3677417039871216 0.5823701109165207 0.5861854387056628\n",
      "24 1.3611432313919067 0.5948628137769995 0.5729101846090022\n",
      "25 1.337950587272644 0.5883245767659078 0.5959344534328977\n",
      "26 1.3099967241287231 0.6035610040863981 0.5756067205973865\n",
      "27 1.3182770013809204 0.5919439579684763 0.6083800041485169\n",
      "28 1.300389051437378 0.6140688849970811 0.5834889027172786\n",
      "29 1.2992782592773438 0.5998832457676591 0.6156399087326281\n",
      "30 1.2592830657958984 0.6239929947460595 0.5787181082762912\n",
      "31 1.2824612855911255 0.5972562755399883 0.6233146650072598\n",
      "32 1.254363775253296 0.6346176298890834 0.602572080481228\n",
      "33 1.2387372255325317 0.6192060712200818 0.6264260526861647\n",
      "34 1.1996289491653442 0.6420315236427321 0.611698817672682\n",
      "35 1.203800082206726 0.6290718038528896 0.630574569591371\n",
      "36 1.1891382932662964 0.6457676590776416 0.6166770379589297\n",
      "37 1.1765906810760498 0.6334500875656742 0.6282928852935076\n",
      "38 1.1572632789611816 0.6466433158201985 0.6255963493051234\n",
      "39 1.1604585647583008 0.6426736719206071 0.6270483302219456\n",
      "40 1.1541979312896729 0.647343841214244 0.6316116988176727\n",
      "41 1.1332777738571167 0.6493870402802101 0.6359676415681393\n",
      "42 1.1217083930969238 0.6551663747810859 0.6374196224849616\n",
      "43 1.113709568977356 0.6583771161704611 0.6413607135449076\n",
      "44 1.1113848686218262 0.6639813193228254 0.6446795270690728\n",
      "45 1.087766408920288 0.6667834208990076 0.6455092304501141\n",
      "46 1.0796291828155518 0.6702276707530648 0.6506948765816221\n",
      "47 1.0722378492355347 0.6747227086981903 0.6529765608794856\n",
      "48 1.070695400238037 0.6785172212492703 0.6548433934868284\n",
      "49 1.0496935844421387 0.6829538820782254 0.6567102260941713\n",
      "50 1.0423119068145752 0.6837711617046118 0.6608587429993777\n",
      "51 1.03616201877594 0.689375364856976 0.6602364654635967\n",
      "52 1.03463876247406 0.6883829538820783 0.6641775565235428\n",
      "53 1.015132188796997 0.6939287799182721 0.6652146857498444\n",
      "54 1.0085172653198242 0.6943957968476357 0.666874092511927\n",
      "55 1.0023943185806274 0.6979568009340339 0.6685334992740095\n",
      "56 1.0014564990997314 0.699883245767659 0.6687409251192699\n",
      "57 0.9824428558349609 0.703677758318739 0.6730968678697365\n",
      "58 0.9772323369979858 0.7050204319906597 0.6747562746318191\n",
      "59 0.9704375863075256 0.7085230589608873 0.6749637004770794\n",
      "60 0.9705175161361694 0.708990075890251 0.6764156813939017\n",
      "61 0.9516013264656067 0.712025685931115 0.6776602364654636\n",
      "62 0.9478267431259155 0.7133683596030356 0.6807716241443684\n",
      "63 0.940363883972168 0.7152948044366608 0.68180875337067\n",
      "64 0.9412920475006104 0.7204319906596615 0.6809790499896287\n",
      "65 0.9229112267494202 0.7194395796847636 0.6849201410495748\n",
      "66 0.9203038215637207 0.7225919439579684 0.6840904376685335\n",
      "67 0.9129732847213745 0.7224751897256275 0.686372121966397\n",
      "68 0.9145295023918152 0.7258610624635143 0.6907280647168638\n",
      "69 0.8982617855072021 0.7275539988324576 0.686372121966397\n",
      "70 0.8972569704055786 0.7319906596614127 0.6944617299315494\n",
      "71 0.8936192393302917 0.7312901342673672 0.6874092511926986\n",
      "72 0.8985447883605957 0.7335084646818447 0.6913503422526447\n",
      "73 0.892265260219574 0.7286631640396964 0.6803567724538477\n",
      "74 0.9012159705162048 0.7312901342673672 0.686994399502178\n",
      "75 0.90805983543396 0.7231173380035026 0.6712300352623937\n",
      "76 0.9236161112785339 0.724927028604787 0.6888612321095209\n",
      "77 0.8978114724159241 0.7259194395796847 0.6807716241443684\n",
      "78 0.8943010568618774 0.736077057793345 0.6994399502177971\n",
      "79 0.8669053912162781 0.7396380618797431 0.6944617299315494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor(3.2220, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.2808, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9778, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8544, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7962, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8289, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8159, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0003, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9040, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8264, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7451, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6693, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6116, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6431, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6351, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.5909, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.5038, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.5018, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.4518, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.4666, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.4643, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.4154, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.3674, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.3677, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.3611, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.3380, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.3100, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.3183, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.3004, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.2993, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.2593, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.2825, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.2544, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.2387, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1996, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.2038, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1891, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1766, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1573, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1605, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1542, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1333, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1217, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1137, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.1114, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0878, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0796, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0722, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0707, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0497, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0423, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0362, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0346, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0151, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0085, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0024, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.0015, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9824, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9772, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9704, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9705, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9516, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9478, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9404, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9413, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9229, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9203, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9130, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9145, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.8983, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.8973, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.8936, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.8985, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.8923, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9012, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9081, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.9236, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.8978, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.8943, grad_fn=<NllLossBackward>),\n",
       "  tensor(0.8669, grad_fn=<NllLossBackward>)],\n",
       " [0.0655575014594279,\n",
       "  0.34740221833041446,\n",
       "  0.45125510799766494,\n",
       "  0.46468184471687096,\n",
       "  0.48488032691185057,\n",
       "  0.4687098657326328,\n",
       "  0.4692936368943374,\n",
       "  0.40758902510215994,\n",
       "  0.43887915936952715,\n",
       "  0.47046117921774666,\n",
       "  0.5076474022183304,\n",
       "  0.508990075890251,\n",
       "  0.5213076474022184,\n",
       "  0.512784588441331,\n",
       "  0.5267367192060712,\n",
       "  0.5405137186223,\n",
       "  0.5425569176882662,\n",
       "  0.5603035610040864,\n",
       "  0.5652072387624051,\n",
       "  0.5642732049036777,\n",
       "  0.5741389375364857,\n",
       "  0.5784004670169294,\n",
       "  0.5817863397548161,\n",
       "  0.5823701109165207,\n",
       "  0.5948628137769995,\n",
       "  0.5883245767659078,\n",
       "  0.6035610040863981,\n",
       "  0.5919439579684763,\n",
       "  0.6140688849970811,\n",
       "  0.5998832457676591,\n",
       "  0.6239929947460595,\n",
       "  0.5972562755399883,\n",
       "  0.6346176298890834,\n",
       "  0.6192060712200818,\n",
       "  0.6420315236427321,\n",
       "  0.6290718038528896,\n",
       "  0.6457676590776416,\n",
       "  0.6334500875656742,\n",
       "  0.6466433158201985,\n",
       "  0.6426736719206071,\n",
       "  0.647343841214244,\n",
       "  0.6493870402802101,\n",
       "  0.6551663747810859,\n",
       "  0.6583771161704611,\n",
       "  0.6639813193228254,\n",
       "  0.6667834208990076,\n",
       "  0.6702276707530648,\n",
       "  0.6747227086981903,\n",
       "  0.6785172212492703,\n",
       "  0.6829538820782254,\n",
       "  0.6837711617046118,\n",
       "  0.689375364856976,\n",
       "  0.6883829538820783,\n",
       "  0.6939287799182721,\n",
       "  0.6943957968476357,\n",
       "  0.6979568009340339,\n",
       "  0.699883245767659,\n",
       "  0.703677758318739,\n",
       "  0.7050204319906597,\n",
       "  0.7085230589608873,\n",
       "  0.708990075890251,\n",
       "  0.712025685931115,\n",
       "  0.7133683596030356,\n",
       "  0.7152948044366608,\n",
       "  0.7204319906596615,\n",
       "  0.7194395796847636,\n",
       "  0.7225919439579684,\n",
       "  0.7224751897256275,\n",
       "  0.7258610624635143,\n",
       "  0.7275539988324576,\n",
       "  0.7319906596614127,\n",
       "  0.7312901342673672,\n",
       "  0.7335084646818447,\n",
       "  0.7286631640396964,\n",
       "  0.7312901342673672,\n",
       "  0.7231173380035026,\n",
       "  0.724927028604787,\n",
       "  0.7259194395796847,\n",
       "  0.736077057793345,\n",
       "  0.7396380618797431],\n",
       " [0.3437046255963493,\n",
       "  0.4517734909769757,\n",
       "  0.45384774942957895,\n",
       "  0.4781165733250363,\n",
       "  0.46069280232316945,\n",
       "  0.4706492428956648,\n",
       "  0.39742791951877204,\n",
       "  0.4463804190002074,\n",
       "  0.46608587429993775,\n",
       "  0.5121344119477287,\n",
       "  0.5086081725783033,\n",
       "  0.5193943165318399,\n",
       "  0.5098527276498652,\n",
       "  0.527276498651732,\n",
       "  0.536403235843186,\n",
       "  0.5420037336652147,\n",
       "  0.5486413607135449,\n",
       "  0.5621240406554656,\n",
       "  0.551337896701929,\n",
       "  0.5706285003111388,\n",
       "  0.562953744036507,\n",
       "  0.5789255341215516,\n",
       "  0.5675171126322339,\n",
       "  0.5861854387056628,\n",
       "  0.5729101846090022,\n",
       "  0.5959344534328977,\n",
       "  0.5756067205973865,\n",
       "  0.6083800041485169,\n",
       "  0.5834889027172786,\n",
       "  0.6156399087326281,\n",
       "  0.5787181082762912,\n",
       "  0.6233146650072598,\n",
       "  0.602572080481228,\n",
       "  0.6264260526861647,\n",
       "  0.611698817672682,\n",
       "  0.630574569591371,\n",
       "  0.6166770379589297,\n",
       "  0.6282928852935076,\n",
       "  0.6255963493051234,\n",
       "  0.6270483302219456,\n",
       "  0.6316116988176727,\n",
       "  0.6359676415681393,\n",
       "  0.6374196224849616,\n",
       "  0.6413607135449076,\n",
       "  0.6446795270690728,\n",
       "  0.6455092304501141,\n",
       "  0.6506948765816221,\n",
       "  0.6529765608794856,\n",
       "  0.6548433934868284,\n",
       "  0.6567102260941713,\n",
       "  0.6608587429993777,\n",
       "  0.6602364654635967,\n",
       "  0.6641775565235428,\n",
       "  0.6652146857498444,\n",
       "  0.666874092511927,\n",
       "  0.6685334992740095,\n",
       "  0.6687409251192699,\n",
       "  0.6730968678697365,\n",
       "  0.6747562746318191,\n",
       "  0.6749637004770794,\n",
       "  0.6764156813939017,\n",
       "  0.6776602364654636,\n",
       "  0.6807716241443684,\n",
       "  0.68180875337067,\n",
       "  0.6809790499896287,\n",
       "  0.6849201410495748,\n",
       "  0.6840904376685335,\n",
       "  0.686372121966397,\n",
       "  0.6907280647168638,\n",
       "  0.686372121966397,\n",
       "  0.6944617299315494,\n",
       "  0.6874092511926986,\n",
       "  0.6913503422526447,\n",
       "  0.6803567724538477,\n",
       "  0.686994399502178,\n",
       "  0.6712300352623937,\n",
       "  0.6888612321095209,\n",
       "  0.6807716241443684,\n",
       "  0.6994399502177971,\n",
       "  0.6944617299315494])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=0,\n",
    "                          output_dim=len(all_labels))\n",
    "train_util(model, X_train_w0, Y_train, X_dev_w0, Y_dev, n_epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for pretrained word embs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
