{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        lines: [['hello', 'world'], ...]\n",
    "        labels: [[!], [N], ...]\n",
    "        vocab\n",
    "    \"\"\"\n",
    "    with open(file, 'rt') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n\\n')\n",
    "    ret_lines = []\n",
    "    labels = []\n",
    "    vocab = set()\n",
    "    for line in lines:\n",
    "        if not line: \n",
    "            continue\n",
    "        curr_line = []\n",
    "        for token_label_str in line.split('\\n'):\n",
    "            if not token_label_str: \n",
    "                continue\n",
    "            token, label = token_label_str.split('\\t')\n",
    "            vocab.add(token)\n",
    "            labels.append(label)\n",
    "            curr_line.append(token)\n",
    "        ret_lines.append(curr_line)\n",
    "    return ret_lines, labels, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lines(lines, word2idx_map, window_size):\n",
    "    \"\"\"\n",
    "    returns X: len(lines) x (2 * window_size + 1)\n",
    "    \"\"\"\n",
    "    def encode_line(line, word2idx_map, window_size):\n",
    "        num_repr = [] # numerical representation\n",
    "        for word in line:\n",
    "            num = word2idx_map.get(word, word2idx_map['UUUNKKK'])\n",
    "            num_repr.append(num)\n",
    "        # pad with start and end tokens\n",
    "        start = [word2idx_map['<s>']] * window_size\n",
    "        end = [word2idx_map['</s>']] * window_size\n",
    "        padded = start + num_repr + end\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(window_size, len(padded) - window_size):\n",
    "            windowed = padded[i - window_size : i + window_size + 1]\n",
    "            ret.append(windowed)\n",
    "            \n",
    "        return ret\n",
    "    \n",
    "    res = []\n",
    "    for line in lines:\n",
    "        res.extend(encode_line(line, word2idx_map, window_size))\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, window_size, output_dim,\n",
    "                 emb_dim=50, pretrained_emb=None, freeze=False):\n",
    "        super(FeedForwardTagger, self).__init__()\n",
    "        if pretrained_emb:\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrain_emb)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        input_dim = (2 * window_size + 1) * emb_dim\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.emb(inputs).squeeze()\n",
    "        out = F.tanh(self.fc1(embeds))\n",
    "        out = self.fc2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util(model, X_train, Y_train, X_dev, Y_dev, n_epochs):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "    losses = []\n",
    "    # TODO train_accu, dev_accu\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(X_train)\n",
    "        loss = loss_func(log_probs, Y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(epoch, loss.item())\n",
    "        losses.append(loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, train_vocab = read_corpus(DATADIR + 'twpos-train.tsv')\n",
    "dev, dev_labels, dev_vocab = read_corpus(DATADIR + 'twpos-dev.tsv')\n",
    "devtest, devtest_labels, devtest_vocab = read_corpus(DATADIR + 'twpos-devtest.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(set(train_labels)))\n",
    "Y_train = label_encoder.transform(train_labels)\n",
    "Y_dev = label_encoder.transform(dev_labels)\n",
    "Y_devtest = label_encoder.transform(devtest_labels)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_dev = torch.tensor(Y_dev, dtype=torch.long)\n",
    "Y_devtest = torch.tensor(Y_devtest, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_vocab.copy()\n",
    "vocab.update(dev_vocab)\n",
    "vocab.update(devtest_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline w/ Randomly Initialized Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for randomly initialized embs\n",
    "idx2word_rand = sorted(vocab)\n",
    "idx2word_rand += ['<s>', '</s>', 'UUUNKKK']\n",
    "word2idx_rand = {word: idx for idx, word in enumerate(idx2word_rand)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Train, Dev, DevTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "X_train_w0 = encode_lines(train, word2idx_rand, window_size=0)\n",
    "X_dev_w0 = encode_lines(dev, word2idx_rand, window_size=0)\n",
    "X_devtest_w0 = encode_lines(devtest, word2idx_rand, window_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 1\n",
    "X_train_w3 = encode_lines(train, word2idx_rand, window_size=1)\n",
    "X_dev_w3 = encode_lines(dev, word2idx_rand, window_size=1)\n",
    "X_devtest_w3 = encode_lines(devtest, word2idx_rand, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.266125202178955\n",
      "1 2.9613800048828125\n",
      "2 2.7306466102600098\n",
      "3 2.5591845512390137\n",
      "4 2.421276807785034\n",
      "5 2.310708522796631\n",
      "6 2.221574544906616\n",
      "7 2.148451328277588\n",
      "8 2.0876646041870117\n",
      "9 2.036442279815674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(3.2661, grad_fn=<NllLossBackward>),\n",
       " tensor(2.9614, grad_fn=<NllLossBackward>),\n",
       " tensor(2.7306, grad_fn=<NllLossBackward>),\n",
       " tensor(2.5592, grad_fn=<NllLossBackward>),\n",
       " tensor(2.4213, grad_fn=<NllLossBackward>),\n",
       " tensor(2.3107, grad_fn=<NllLossBackward>),\n",
       " tensor(2.2216, grad_fn=<NllLossBackward>),\n",
       " tensor(2.1485, grad_fn=<NllLossBackward>),\n",
       " tensor(2.0877, grad_fn=<NllLossBackward>),\n",
       " tensor(2.0364, grad_fn=<NllLossBackward>)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FeedForwardTagger(vocab_size=len(word2idx_rand), \n",
    "                          window_size=0,\n",
    "                          output_dim=len(all_labels))\n",
    "train_util(model, X_train_w0, Y_train, X_dev_w0, Y_dev, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct maps for pretrained word embs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
